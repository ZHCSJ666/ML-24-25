{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5UUGZ0AdpuO",
        "outputId": "f974125d-cba7-4b52-fc09-842e67cf2158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating all possible combinations. This may take a while...\n",
            "Total combinations generated: 832\n",
            "Dataset generated and saved to commit_dataset.json\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "function_names = [\n",
        "    \"display_greeting\", \"show_warning\", \"print_info\", \"log_event\",\n",
        "    \"announce_start\", \"notify_completion\", \"output_status\", \"send_alert\",\n",
        "    # \"record_activity\", \"trace_execution\", \"emit_signal\", \"broadcast_message\",\n",
        "    # \"report_error\", \"inform_user\", \"update_display\", \"refresh_view\",\n",
        "    # \"render_output\", \"present_cdata\", \"show_notification\", \"display_result\",\n",
        "    # \"print_summary\", \"log_details\", \"notify_admin\", \"output_log\",\n",
        "    # \"send_update\", \"record_log\", \"trace_process\", \"emit_event\", \"broadcast_alert\",\n",
        "    # \"report_status\", \"inform_system\",\n",
        "    # \"initialize_module\", \"shutdown_service\", \"handle_request\", \"process_data\",\n",
        "    # \"validate_input\", \"authenticate_user\", \"authorize_access\", \"compress_files\",\n",
        "    # \"decompress_files\", \"backup_database\", \"restore_database\", \"sync_files\",\n",
        "    # \"monitor_performance\", \"optimize_queries\", \"generate_report\", \"send_email\",\n",
        "    # \"schedule_task\", \"cancel_task\", \"retry_operation\", \"log_transaction\",\n",
        "    # \"update_configuration\", \"load_settings\", \"parse_response\", \"manage_sessions\",\n",
        "    # \"encrypt_data\", \"decrypt_data\", \"format_output\", \"validate_credentials\",\n",
        "    # \"handle_errors\", \"process_payments\", \"manage_notifications\", \"track_usage\",\n",
        "    # \"generate_token\", \"verify_identity\", \"log_out\", \"register_user\"\n",
        "]\n",
        "\n",
        "printed_messages = [\n",
        "    \"Hello, World!\", \"Warning: Low Disk Space.\", \"Information: Process started.\",\n",
        "    \"Event logged successfully.\", \"Starting the application.\", \"Completion successful.\",\n",
        "    \"Status: All systems operational.\", \"Alert: Unauthorized access detected.\",\n",
        "    # \"Activity recorded.\", \"Execution trace started.\", \"Signal emitted.\",\n",
        "    # \"Message broadcasted.\", \"Error encountered in module.\", \"User informed.\",\n",
        "    # \"Display updated.\", \"View refreshed.\", \"Output rendered.\", \"Data presented.\",\n",
        "    # \"Notification displayed.\", \"Result shown.\", \"Summary printed.\",\n",
        "    # \"Details logged.\", \"Admin notified.\", \"Log output generated.\",\n",
        "    # \"Update sent to server.\", \"Log recorded.\", \"Process traced.\", \"Event emitted.\",\n",
        "    # \"Alert broadcasted.\", \"Status reported.\", \"System informed.\",\n",
        "    # \"Module initialized.\", \"Service shutdown gracefully.\", \"Request handled successfully.\",\n",
        "    # \"Data processed without errors.\", \"Input validated.\", \"User authenticated.\",\n",
        "    # \"Access authorized.\", \"Files compressed.\", \"Files decompressed.\",\n",
        "    # \"Database backed up.\", \"Database restored.\", \"Files synchronized.\",\n",
        "    # \"Performance monitored.\", \"Queries optimized.\", \"Report generated.\",\n",
        "    # \"Email sent successfully.\", \"Task scheduled.\", \"Task canceled.\",\n",
        "    # \"Operation retried.\", \"Transaction logged.\", \"Configuration updated.\",\n",
        "    # \"Settings loaded.\", \"Response parsed.\", \"Session managed.\",\n",
        "    # \"Data encrypted.\", \"Data decrypted.\", \"Output formatted.\", \"Credentials validated.\",\n",
        "    # \"Errors handled.\", \"Payments processed.\", \"Notifications managed.\",\n",
        "    # \"Usage tracked.\", \"Token generated.\", \"Identity verified.\", \"User logged out.\",\n",
        "    # \"User registered successfully.\"\n",
        "]\n",
        "\n",
        "file_names = [\n",
        "    \"utils.py\", \"helpers.py\", \"main.py\", \"scripts.py\", \"commands.py\",\n",
        "    \"logger.py\", \"notifications.py\", \"functions.py\", \"actions.py\",\n",
        "    \"alerts.py\", \"outputs.py\", \"interactions.py\", \"communication.py\",\n",
        "    # \"handlers.py\", \"response.py\", \"initializer.py\", \"setup.py\", \"runner.py\",\n",
        "    # \"manager.py\", \"processor.py\", \"controller.py\", \"service.py\", \"adapter.py\",\n",
        "    # \"connector.py\", \"dispatcher.py\", \"executor.py\", \"facade.py\", \"gateway.py\",\n",
        "    # \"handler.py\", \"integrator.py\", \"mediator.py\", \"observer.py\", \"provider.py\",\n",
        "    # \"registrar.py\", \"scheduler.py\", \"translator.py\", \"validator.py\", \"watcher.py\",\n",
        "    # \"database.py\", \"authentication.py\", \"authorization.py\", \"backup.py\",\n",
        "    # \"monitor.py\", \"reporting.py\", \"email_service.py\", \"task_manager.py\",\n",
        "    # \"transaction.py\", \"compression.py\", \"decompression.py\", \"synchronization.py\",\n",
        "    # \"performance.py\", \"optimization.py\",\n",
        "    # \"config.py\", \"utils_v2.py\", \"helpers_v2.py\", \"main_v2.py\",\n",
        "    # \"scripts_v2.py\", \"commands_v2.py\", \"logger_v2.py\", \"notifications_v2.py\",\n",
        "    # \"functions_v2.py\", \"actions_v2.py\", \"alerts_v2.py\", \"outputs_v2.py\",\n",
        "    # \"interactions_v2.py\", \"communication_v2.py\", \"handlers_v2.py\",\n",
        "    # \"response_v2.py\", \"initializer_v2.py\", \"setup_v2.py\", \"runner_v2.py\",\n",
        "    # \"manager_v2.py\", \"processor_v2.py\", \"controller_v2.py\", \"service_v2.py\",\n",
        "    # \"adapter_v2.py\", \"connector_v2.py\", \"dispatcher_v2.py\", \"executor_v2.py\",\n",
        "    # \"facade_v2.py\", \"gateway_v2.py\", \"handler_v2.py\", \"integrator_v2.py\",\n",
        "    # \"mediator_v2.py\", \"observer_v2.py\", \"provider_v2.py\", \"registrar_v2.py\",\n",
        "    # \"scheduler_v2.py\", \"translator_v2.py\", \"validator_v2.py\", \"watcher_v2.py\",\n",
        "    # \"database_v2.py\", \"authentication_v2.py\", \"authorization_v2.py\",\n",
        "    # \"backup_v2.py\", \"monitor_v2.py\", \"reporting_v2.py\", \"email_service_v2.py\",\n",
        "    # \"task_manager_v2.py\", \"transaction_v2.py\", \"compression_v2.py\",\n",
        "    # \"decompression_v2.py\", \"synchronization_v2.py\", \"performance_v2.py\",\n",
        "    # \"optimization_v2.py\"\n",
        "]\n",
        "\n",
        "def generate_file_content(function_name, message):\n",
        "    \"\"\"\n",
        "    Generates the complete content of a Python file by concatenating\n",
        "    the new function definition.\n",
        "    \"\"\"\n",
        "    function_def = (\n",
        "        f\"def {function_name}():\\n\"\n",
        "        f\"    print(\\\"{message}\\\")\\n\"\n",
        "    )\n",
        "    return function_def\n",
        "\n",
        "def generate_commit_message(message):\n",
        "    return f\"Added function to print \\\"{message}\\\"\"\n",
        "\n",
        "def generate_all_combinations():\n",
        "    dataset = []\n",
        "    file_version = {file_name: 0 for file_name in file_names}  # Initialize version tracking\n",
        "\n",
        "    for file_name, function_name, message in itertools.product(file_names, function_names, printed_messages):\n",
        "        file_version[file_name] += 1\n",
        "        version = file_version[file_name]\n",
        "\n",
        "        file_content = generate_file_content(function_name, message)\n",
        "\n",
        "        commit_message = generate_commit_message(message)\n",
        "\n",
        "        dataset.append({\n",
        "            \"file_name\": file_name,\n",
        "            \"version\": version,\n",
        "            \"commit_diff\": file_content,\n",
        "            \"commit_message\": commit_message\n",
        "        })\n",
        "\n",
        "    return dataset\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Generating all possible combinations. This may take a while...\")\n",
        "    dataset = generate_all_combinations()\n",
        "    print(f\"Total combinations generated: {len(dataset)}\")\n",
        "    with open(\"commit_dataset.json\", \"w\") as f:\n",
        "        json.dump(dataset, f, indent=2)\n",
        "    print(\"Dataset generated and saved to commit_dataset.json\")\n",
        "    df = pd.DataFrame(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6snvDwjBtjG0",
        "outputId": "7486364f-c560-4a16-b5f5-907fe5833163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Training samples: 666\n",
            "Testing samples: 166\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 | Train Loss: 0.2192 | Val Loss: 0.0010\n",
            "Epoch 2/10 | Train Loss: 0.0013 | Val Loss: 0.0004\n",
            "Epoch 3/10 | Train Loss: 0.0006 | Val Loss: 0.0002\n",
            "Epoch 4/10 | Train Loss: 0.0003 | Val Loss: 0.0001\n",
            "Epoch 5/10 | Train Loss: 0.0002 | Val Loss: 0.0001\n",
            "Epoch 6/10 | Train Loss: 0.0002 | Val Loss: 0.0001\n",
            "Epoch 7/10 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 8/10 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 9/10 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 10/10 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Commit Diff:\n",
            "def list_users():\n",
            "    \"\"\"Lists all users in the system.\"\"\"\n",
            "    return database.find_all()\n",
            "\n",
            "\n",
            "Generated Commit Message: Added function print print print to print.\".\" \"Starting!\" \" \" print.\" print.\"Added print function function functionStarting function print print function print function, function function to print printHello print print\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "\n",
        "commit_diffs = df['commit_diff'].tolist() + df['commit_message'].tolist()\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
        "tokenizer.decoder = decoders.ByteLevel()\n",
        "\n",
        "special_tokens = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
        "\n",
        "trainer = trainers.BpeTrainer(vocab_size=3000, special_tokens=special_tokens)\n",
        "\n",
        "tokenizer.train_from_iterator(commit_diffs, trainer=trainer)\n",
        "\n",
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"<s> $A </s>\",\n",
        "    pair=\"<s> $A </s> </s> $B </s>\",\n",
        "    special_tokens=[\n",
        "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    ],\n",
        ")\n",
        "\n",
        "tokenizer.save(\"bpe_tokenizer.json\")\n",
        "\n",
        "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
        "\n",
        "def encode_text(text, max_length):\n",
        "    encoding = tokenizer.encode(text)\n",
        "    if len(encoding.ids) > max_length:\n",
        "        encoding = encoding.slice(0, max_length)\n",
        "    else:\n",
        "        encoding.pad(length=max_length)\n",
        "    return encoding.ids\n",
        "\n",
        "max_input_length = 100\n",
        "max_target_length = 100\n",
        "\n",
        "input_ids = [encode_text(diff, max_input_length) for diff in df['commit_diff'].tolist()]\n",
        "target_ids = [encode_text(msg, max_target_length) for msg in df['commit_message'].tolist()]\n",
        "\n",
        "input_ids = torch.tensor(input_ids)\n",
        "target_ids = torch.tensor(target_ids)\n",
        "\n",
        "dataset_size = len(df)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(0.2 * dataset_size))\n",
        "random.seed(42)\n",
        "random.shuffle(indices)\n",
        "train_indices, test_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_inputs, train_targets = input_ids[train_indices], target_ids[train_indices]\n",
        "test_inputs, test_targets = input_ids[test_indices], target_ids[test_indices]\n",
        "\n",
        "print(f'Training samples: {len(train_inputs)}')\n",
        "print(f'Testing samples: {len(test_inputs)}')\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = nn.Parameter(self._generate_positional_encoding(5000, embed_size), requires_grad=False)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def _generate_positional_encoding(self, max_len, embed_size):\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2) * (-np.log(10000.0) / embed_size))\n",
        "        pe = torch.zeros(max_len, embed_size)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        src = self.token_embedding(src) + self.positional_encoding[:src.size(1), :]\n",
        "        src = self.dropout(src)\n",
        "        memory = self.transformer_encoder(src.permute(1, 0, 2), src_mask)\n",
        "        output = self.fc_out(memory.permute(1, 0, 2))\n",
        "        return output\n",
        "\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "embed_size = 256\n",
        "num_heads = 8\n",
        "hidden_dim = 512\n",
        "num_layers = 3\n",
        "dropout = 0.1\n",
        "\n",
        "model = TransformerModel(vocab_size, embed_size, num_heads, hidden_dim, num_layers, dropout).to(device)\n",
        "\n",
        "class CommitDataset(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.inputs[idx],\n",
        "            'target_ids': self.targets[idx]\n",
        "        }\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "train_dataset = CommitDataset(train_inputs, train_targets)\n",
        "test_dataset = CommitDataset(test_inputs, test_targets)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "pad_token_id = tokenizer.token_to_id(\"<pad>\")\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        target_ids = batch['target_ids'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_ids, src_mask=None)\n",
        "\n",
        "        output = output.contiguous().view(-1, vocab_size)  # (batch_size * seq_len, vocab_size)\n",
        "        target = target_ids.contiguous().view(-1)          # (batch_size * seq_len)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            target_ids = batch['target_ids'].to(device)\n",
        "\n",
        "            output = model(input_ids, src_mask=None)\n",
        "\n",
        "            output = output.contiguous().view(-1, vocab_size)  # (batch_size * seq_len, vocab_size)\n",
        "            target = target_ids.contiguous().view(-1)          # (batch_size * seq_len)\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate(model, test_loader, criterion, device)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
        "\n",
        "def generate_commit_message(model, tokenizer, commit_diff, max_length=64):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        encoding = tokenizer.encode(commit_diff)\n",
        "        if len(encoding.ids) > max_input_length:\n",
        "            encoding = encoding.slice(0, max_input_length)\n",
        "        else:\n",
        "            encoding.pad(length=max_length)  # Pad in place\n",
        "        input_ids = torch.tensor([encoding.ids]).to(device)\n",
        "\n",
        "        outputs = model(input_ids)\n",
        "        predicted_ids = outputs.argmax(dim=-1).squeeze().cpu().numpy()\n",
        "\n",
        "        decoded = tokenizer.decode(predicted_ids)\n",
        "        decoded = decoded.replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n",
        "        return decoded\n",
        "\n",
        "new_commit_diff = '''def list_users():\n",
        "    \"\"\"Lists all users in the system.\"\"\"\n",
        "    return database.find_all()\n",
        "'''\n",
        "\n",
        "generated_message = generate_commit_message(model, tokenizer, new_commit_diff)\n",
        "print(f'Commit Diff:\\n{new_commit_diff}\\n')\n",
        "print(f'Generated Commit Message: {generated_message}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVjahV7MfBh6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
