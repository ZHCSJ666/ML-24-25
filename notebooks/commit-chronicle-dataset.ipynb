{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c942aa44c0386e52",
   "metadata": {},
   "source": [
    "# Commit Chronicle Dataset\n",
    "\n",
    "This notebook investigates the [Commit Chronicle dataset](https://huggingface.co/datasets/JetBrains-Research/commit-chronicle) introduced in the paper [\"From Commit Message Generation to History-Aware Commit Message Completion\", ASE 2023](https://arxiv.org/abs/2308.07655) - loading, filtering, EDA and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "import rootutils\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8947f90d5c308fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = rootutils.setup_root(\".\", \".project-root\", pythonpath=True)\n",
    "OUTPUT_DIR = ROOT / \"data/playground\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929531a0d4f45abe",
   "metadata": {},
   "source": [
    "## Loading and Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d52de09eaa69",
   "metadata": {},
   "source": [
    "Note: Filtering logic is implemented in `CommitChroniclePreprocessor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6a878de398ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to continue with the rest of this notebook.\n",
    "\n",
    "SPLIT = \"validation\"  # we select this split as it's small for our EDA, feel free to change to `train` split if u want\n",
    "LANGUAGES = [\"Go\"]\n",
    "\n",
    "filtered = OUTPUT_DIR / \"01-filtered-validation\"\n",
    "\n",
    "\n",
    "# we don't directly reference `LANGUAGES` in the function because in python multiprocessing,\n",
    "# all functions passed as parameters shouldn't reference variables outside of them\n",
    "def filter_dataset(example, languages):\n",
    "    return example[\"language\"] in languages\n",
    "\n",
    "\n",
    "if not filtered.exists():\n",
    "    (\n",
    "        load_dataset(\"JetBrains-Research/commit-chronicle\", \"default\", split=SPLIT)\n",
    "        .filter(partial(filter_dataset, languages=LANGUAGES), num_proc=mp.cpu_count())\n",
    "        .save_to_disk(filtered)\n",
    "    )\n",
    "dataset = load_from_disk(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc8682053d43e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.select(range(10)).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8818a3cf2413f69",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e0a1a",
   "metadata": {},
   "source": [
    "### Column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae7054",
   "metadata": {},
   "source": [
    "These are the columns we have in our dataset and an example of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8476249c9784e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    dataset.column_names\n",
    ")  # ['author','date','timezone','hash','message','mods','language','license','repo','original_message']\n",
    "\n",
    "subset = dataset.select(range(10))\n",
    "\n",
    "for element in subset:\n",
    "    print(element)\n",
    "\n",
    "subset[0][\n",
    "    \"mods\"\n",
    "]  # 'Mods' will have multiple changes in different files. need to be appended accordingly. Its length varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()\n",
    "add_changes_df = df[\n",
    "    df[\"mods\"].apply(lambda mods: all(mod[\"change_type\"] == \"ADD\" for mod in mods))\n",
    "]\n",
    "print(\n",
    "    len(add_changes_df)\n",
    ")  # we have 3400 rows of changes that ONLY contain ADD type, can be used as a start to train our first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e05e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_changes_df = df[\n",
    "    df[\"mods\"].apply(lambda mods: all(mod[\"change_type\"] == \"MODIFY\" for mod in mods))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a369c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_change_types(df):\n",
    "    \"\"\"\n",
    "    Plots the distribution of change types (e.g., MODIFY, ADD, DELETE).\n",
    "    \"\"\"\n",
    "    change_types = (\n",
    "        df[\"mods\"]\n",
    "        .apply(lambda mods: [mod[\"change_type\"] for mod in mods])\n",
    "        .explode()\n",
    "        .value_counts()\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=change_types.values, y=change_types.index, palette=\"coolwarm\")\n",
    "    plt.title(\"Distribution of Change Types\")\n",
    "    plt.xlabel(\"Number of Changes\")\n",
    "    plt.ylabel(\"Change Type\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_change_types(df)  # We have a lot of MODIFY changes, ADD and DELETE are less frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb32aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def analyze_commit_messages(df):\n",
    "    \"\"\"\n",
    "    Analyzes and plots the most common words in commit messages.\n",
    "    \"\"\"\n",
    "    # Combine all messages\n",
    "    all_messages = \" \".join(df[\"message\"].dropna().tolist())\n",
    "\n",
    "    # Remove punctuation\n",
    "    all_messages = all_messages.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # Tokenize and count\n",
    "    words = all_messages.lower().split()\n",
    "    common_words = Counter(words).most_common(20)\n",
    "\n",
    "    # Plot\n",
    "    words, counts = zip(*common_words)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=list(words), y=list(counts), hue=list(words), legend=False)\n",
    "    plt.title(\"Top 20 Common Words in Commit Messages\")\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "analyze_commit_messages(df)\n",
    "analyze_commit_messages(add_changes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750870b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    words = text.split()\n",
    "    stopwords_list = nltk.corpus.stopwords.words(\"english\")\n",
    "    stop_words = set(stopwords_list)\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return cleaned_words\n",
    "\n",
    "\n",
    "def get_word_counts(df):\n",
    "    all_words = []\n",
    "    for message in df[\"message\"].dropna():\n",
    "        all_words.extend(preprocess_text(message))\n",
    "    return Counter(all_words)\n",
    "\n",
    "\n",
    "word_counts_original = get_word_counts(\n",
    "    df.sample(len(add_changes_df))\n",
    ")  # sample the same number of ADD changes to get a good idea\n",
    "word_counts_add = get_word_counts(add_changes_df)\n",
    "word_counts_modify = get_word_counts(modify_changes_df.sample(len(add_changes_df)))\n",
    "\n",
    "\n",
    "def counter_to_df(counter, title):\n",
    "    df = pd.DataFrame(counter.most_common(20), columns=[\"word\", \"count\"])\n",
    "    df[\"dataset\"] = title\n",
    "    return df\n",
    "\n",
    "\n",
    "df_original_words = counter_to_df(word_counts_original, \"All Commits\")\n",
    "df_add_words = counter_to_df(word_counts_add, \"Only ADD Commits\")\n",
    "df_modify_words = counter_to_df(word_counts_modify, \"Only MODIFY Commits\")\n",
    "\n",
    "df_combined = pd.concat([df_original_words, df_add_words, df_modify_words])\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=df_combined, x=\"word\", y=\"count\", hue=\"dataset\")\n",
    "\n",
    "plt.title(\"Top 20 Common Words in Commit Messages\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend(title=\"Dataset\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()  # We can see there is a difference between the most common words in ADD and MODIFY commits\n",
    "# It's supported by the fact that MODIFY commits are more similar to the original commits as they are more frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a41fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "vocabulary = set(word_counts_original.keys()).union(set(word_counts_add.keys()))\n",
    "\n",
    "counts_original = []\n",
    "counts_add = []\n",
    "\n",
    "for word in vocabulary:\n",
    "    counts_original.append(word_counts_original.get(word, 0))\n",
    "    counts_add.append(word_counts_add.get(word, 0))\n",
    "\n",
    "counts_original = np.array(counts_original, dtype=np.float64)\n",
    "counts_add = np.array(counts_add, dtype=np.float64)\n",
    "\n",
    "epsilon = 1e-10\n",
    "counts_original += epsilon\n",
    "counts_add += epsilon\n",
    "\n",
    "prob_original = counts_original / counts_original.sum()\n",
    "prob_add = counts_add / counts_add.sum()\n",
    "\n",
    "kl_divergence = entropy(prob_original, prob_add)\n",
    "\n",
    "print(f\"KL Divergence (All Commits || Only ADD Commits): {kl_divergence:.4f}\")\n",
    "\n",
    "kl_divergence_reverse = entropy(prob_add, prob_original)\n",
    "print(f\"KL Divergence (Only ADD Commits || All Commits): {kl_divergence_reverse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db1b6c",
   "metadata": {},
   "source": [
    "Higher values for the KL Divergence (non-zero) indicate strong differences between both distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35995929c58ee47",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Columns of interest in the dataset are:\n",
    "1. `mods` - Contains all file changes information - what files are changed, the type of change made (addition, modification), and the exact file changes.\n",
    "2. `message` - The (processed) git commit message\n",
    "3. `author` - (Optional) This will be used if we want to group commits by a certain author and use that as input. This is and advanced use case\n",
    "\n",
    "We are going to tokenize the `mods` and `message` using two different tokenizer, since `mods` contains code and which is quite different from `message` which is mostly natural language. So, one tokenizer for `mods`, another for `message`.\n",
    "\n",
    "We'll start with `message`. The output from the tokenization of  `message` will be called `msg_input_ids`.\n",
    "\n",
    "Note: All the preprocessing logic explored here is implemented in `CommitChroniclePreprocessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab50d4730d452e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.data.components.tokenization import add_special_tokens\n",
    "\n",
    "# This is the tokenizer used in the Commit Chronicle dataset\n",
    "# The rationale behind this choice is yet to be investigated? Someone could investigate and report :)\n",
    "# OR we may have to train our own tokenizer as suggested by our all-knowing ChatGPT (https://chatgpt.com/share/672e3b64-6b84-8009-a6c9-adac73cf647e)\n",
    "msg_tokenizer_ = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
    "\n",
    "# add `sep_token` and `pad_token`\n",
    "# `sep_token` is necessary when we are training on a history of git diffs (which is an advanced usage and not part of our initial experiments)\n",
    "# `pad_token` is necessary for correct batch construction.\n",
    "msg_tokenizer_ = add_special_tokens(msg_tokenizer_, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf487d1a2e3f70",
   "metadata": {},
   "source": [
    "Let's try out commit message tokenization on a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f0f8ecb12e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_input_ids_ = msg_tokenizer_(\n",
    "    dataset[0][\"message\"], truncation=False, padding=False, add_special_tokens=False\n",
    ").input_ids\n",
    "\n",
    "print(dataset[0][\"message\"])\n",
    "print(msg_input_ids_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a06a74e9697ee6",
   "metadata": {},
   "source": [
    "Next, we'll look at the tokenization of git commit changes, `mods`. But before we do that, let's examine the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1652316e913146be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][\"mods\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5e4e5b3809ba2a",
   "metadata": {},
   "source": [
    "We'll need to somehow combine all that information into a single string before tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55240fbafad8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mods(mods: list[dict[str, str]], line_sep: str) -> str:\n",
    "    \"\"\"\n",
    "    Transforms a list of all files modifications made in a commit into a single string representation.\n",
    "\n",
    "    Specifically, adds a header to each file diff (https://git-scm.com/docs/git-diff#_generating_patch_text_with_p)\n",
    "    and concatenates the results.\n",
    "\n",
    "    Args:\n",
    "        mods: A list of files modifications made in a commit.\n",
    "        line_sep: The line separator to separate each file modification.\n",
    "\n",
    "    Returns:\n",
    "        A single string representation of all files modifications made in a commit.\n",
    "    \"\"\"\n",
    "    diff = \"\"\n",
    "\n",
    "    for mod in mods:\n",
    "        if mod[\"change_type\"] == \"UNKNOWN\":\n",
    "            continue\n",
    "        elif mod[\"change_type\"] == \"ADD\":\n",
    "            file_diff = f\"new file {mod['new_path']}\"\n",
    "        elif mod[\"change_type\"] == \"DELETE\":\n",
    "            file_diff = f\"deleted file {mod['old_path']}\"\n",
    "        elif mod[\"change_type\"] == \"RENAME\":\n",
    "            file_diff = f\"rename from {mod['old_path']}{line_sep}rename to {mod['new_path']}\"\n",
    "        elif mod[\"change_type\"] == \"COPY\":\n",
    "            file_diff = f\"copy from {mod['old_path']}{line_sep}copy to {mod['new_path']}\"\n",
    "        else:\n",
    "            file_diff = f\"{mod['new_path']}\"\n",
    "        diff += file_diff + line_sep + mod[\"diff\"]\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "# Let's test it out\n",
    "print(preprocess_mods(dataset[0][\"mods\"], line_sep=\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed10213cf9ba57b",
   "metadata": {},
   "source": [
    "Now onto tokenization of the concatenated git diff or `mods`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d1f7e44faf6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Here, we just duplicate the message tokenizer, but it could be completely different, or maybe I lied :)\n",
    "diff_tokenizer_ = deepcopy(msg_tokenizer_)\n",
    "# diff can be very long, we need to set a limit that our model (and computer resources) can handle\n",
    "DIFF_MAX_LEN = 512\n",
    "\n",
    "# again, let's test it\n",
    "git_diff_ = preprocess_mods(dataset[0][\"mods\"], line_sep=\"\\n\")\n",
    "diff_input_ids_ = diff_tokenizer_(\n",
    "    git_diff_,\n",
    "    truncation=True,\n",
    "    max_length=DIFF_MAX_LEN\n",
    "    - 2,  # -2 to account for special tokens (BOS and EOS) to be added later, during batch data construction.\n",
    "    padding=False,\n",
    "    add_special_tokens=False,\n",
    ").input_ids\n",
    "print(diff_input_ids_[:100], len(diff_input_ids_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88139463173dfc1d",
   "metadata": {},
   "source": [
    "Let's put everything together to process `mods` and `message` columns for all rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d987b1978ede38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(\n",
    "    example,\n",
    "    msg_tokenizer,\n",
    "    diff_tokenizer,\n",
    "    diff_max_len,\n",
    "    diff_line_sep,\n",
    "    preprocess_mods_func,\n",
    "):\n",
    "    msg_input_ids = msg_tokenizer(\n",
    "        example[\"message\"], truncation=False, padding=False, add_special_tokens=False\n",
    "    ).input_ids\n",
    "\n",
    "    git_diff = preprocess_mods_func(example[\"mods\"], line_sep=diff_line_sep)\n",
    "    diff_input_ids = diff_tokenizer(\n",
    "        git_diff,\n",
    "        truncation=True,  # we unfortunately have to truncate the git changes\n",
    "        max_length=diff_max_len\n",
    "        - 2,  # -2 to account for special tokens (BOS and EOS) to be added later, during batch data construction.\n",
    "        padding=False,\n",
    "        add_special_tokens=False,\n",
    "    ).input_ids\n",
    "\n",
    "    return {\n",
    "        \"author\": example[\"author\"],\n",
    "        \"message\": example[\"message\"],\n",
    "        \"msg_input_ids\": msg_input_ids,\n",
    "        \"diff_input_ids\": diff_input_ids,\n",
    "        \"repo\": example[\"repo\"],\n",
    "        \"language\": example[\"language\"],\n",
    "    }\n",
    "\n",
    "\n",
    "processed = OUTPUT_DIR / \"02-processed-validation\"\n",
    "if not processed.exists():\n",
    "    (\n",
    "        dataset.map(\n",
    "            partial(\n",
    "                process_example,\n",
    "                msg_tokenizer=msg_tokenizer_,\n",
    "                diff_tokenizer=diff_tokenizer_,\n",
    "                diff_max_len=DIFF_MAX_LEN,\n",
    "                diff_line_sep=\"\\n\",\n",
    "                preprocess_mods_func=preprocess_mods,\n",
    "            ),\n",
    "            num_proc=mp.cpu_count(),\n",
    "        )\n",
    "        .select_columns([\"author\", \"msg_input_ids\", \"diff_input_ids\", \"language\", \"repo\"])\n",
    "        .save_to_disk(processed)\n",
    "    )\n",
    "dataset = load_from_disk(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20298f8df7a86574",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.select(range(10)).to_pandas()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
