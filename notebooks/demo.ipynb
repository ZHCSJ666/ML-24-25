{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Demos\n",
    "\n",
    "This notebook demonstrates a comparison between our trained model and two baselines on some simple demos. The baselines are:\n",
    "- A baseline model (JetBrains-Research/cmg-codet5-without-history)\n",
    "- DeepSeek V3"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T16:39:22.312931Z",
     "start_time": "2025-01-28T16:39:20.066706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import rootutils\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T16:39:23.198423Z",
     "start_time": "2025-01-28T16:39:23.193554Z"
    }
   },
   "cell_type": "code",
   "source": "ROOT = rootutils.setup_root(\".\", \".project-root\", pythonpath=True)",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T16:39:27.763781Z",
     "start_time": "2025-01-28T16:39:23.664834Z"
    }
   },
   "cell_type": "code",
   "source": "from src.demo_inference import load_run, run_inference, fetch_git_changes, run_inference_llm",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T16:39:34.193077Z",
     "start_time": "2025-01-28T16:39:27.768786Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "t5_efficient_extra_tiny_module, t5_efficient_extra_tiny_datamodule = load_run(\n",
    "     checkpoint_path=ROOT / \"logs/train/runs/2025-01-24_23-12-54/checkpoints/epoch_023-val_MRR_top5_0.6524.ckpt\",\n",
    "    config_path=None,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "codet5_module, codet5_datamodule = load_run(\n",
    "     checkpoint_path=None,\n",
    "    config_path=ROOT / \"logs/eval/runs/2025-01-25_11-45-44/.hydra/config.yaml\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "deepseek_v3_module, deepseek_v3_datamodule = load_run(\n",
    "     checkpoint_path=None,\n",
    "    config_path=ROOT / \"logs/eval/runs/2025-01-27_10-31-09/.hydra/config.yaml\",\n",
    "    device=device,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sesugh\\Desktop\\src\\commit-message-generation\\src\\demo_inference.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))  # nosec B614\n",
      "\u001B[32m2025-01-28 17:39:33.172\u001B[0m | \u001B[33m\u001B[1mWARNING \u001B[0m | \u001B[36msrc.demo_inference\u001B[0m:\u001B[36mload_run\u001B[0m:\u001B[36m61\u001B[0m - \u001B[33m\u001B[1mNo tokenizer param in datamodule. Maybe you need to specify a tokenizer\u001B[0m\n",
      "\u001B[32m2025-01-28 17:39:34.190\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.data.commit_chronicle.llm_api\u001B[0m:\u001B[36msetup\u001B[0m:\u001B[36m157\u001B[0m - \u001B[1mTotal tokens in dataset: 10710853 Tokens per sample: 650.641052120034\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example 1: Hello World"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T16:53:37.878353Z",
     "start_time": "2025-01-28T16:53:37.785695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "changes = fetch_git_changes(ROOT / \"data/demos/01-hello-world\")\n",
    "changes"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'change_type': 'MODIFY',\n",
       "  'old_path': 'main.go',\n",
       "  'new_path': 'main.go',\n",
       "  'diff': 'diff --git a/main.go b/main.go\\nindex 1b5cdd1..b2f344e 100644\\n--- a/main.go\\n+++ b/main.go\\n@@ -7,10 +7,13 @@ type User struct {\\n}\\nfunc GetUserName(user *User) string {\\n- return user.Name // BUG: If `user` is nil, this will panic!\\n+ if user == nil {\\n+ return \"Unknown User\" // FIX: Handle the nil case\\n+ }\\n+ return user.Name\\n}\\nfunc main() {\\nvar user *User // user is nil\\n- fmt.Println(\"User Name:\", GetUserName(user)) // This will cause a runtime panic!\\n+ fmt.Println(\"User Name:\", GetUserName(user)) // Now it won\\'t panic!\\n}\\n'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Our Model: T5 Efficient Extra Tiny"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T16:53:41.471087Z",
     "start_time": "2025-01-28T16:53:40.868112Z"
    }
   },
   "cell_type": "code",
   "source": "run_inference(t5_efficient_extra_tiny_module, t5_efficient_extra_tiny_datamodule, changes, device)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               input  \\\n",
       "0  main.go diff --git a/main.go b/main.go index 1...   \n",
       "\n",
       "                                  prediction  \n",
       "0  Refactor GetUserName to return nil string  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main.go diff --git a/main.go b/main.go index 1...</td>\n",
       "      <td>Refactor GetUserName to return nil string</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### CMG CodeT5 Model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T16:53:47.242178Z",
     "start_time": "2025-01-28T16:53:46.751926Z"
    }
   },
   "cell_type": "code",
   "source": "run_inference(codet5_module, codet5_datamodule, changes, device)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               input            prediction\n",
       "0  main.go\\ndiff --git a/main.go b/main.go\\nindex...  Fix panic in main.go"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main.go\\ndiff --git a/main.go b/main.go\\nindex...</td>\n",
       "      <td>Fix panic in main.go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### DeepSeek V3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T16:53:52.846906Z",
     "start_time": "2025-01-28T16:53:49.829920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = run_inference_llm(deepseek_v3_module, deepseek_v3_datamodule, changes)\n",
    "display(output)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                      system_content  \\\n",
       "0  You are a helpful assistant that generates com...   \n",
       "\n",
       "                                        user_content  \\\n",
       "0  Code changes:\\nmain.go\\ndiff --git a/main.go b...   \n",
       "\n",
       "                                                diff  \\\n",
       "0  main.go\\ndiff --git a/main.go b/main.go\\nindex...   \n",
       "\n",
       "                                prediction  \n",
       "0  Fix GetUserName to handle nil user case  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_content</th>\n",
       "      <th>user_content</th>\n",
       "      <th>diff</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are a helpful assistant that generates com...</td>\n",
       "      <td>Code changes:\\nmain.go\\ndiff --git a/main.go b...</td>\n",
       "      <td>main.go\\ndiff --git a/main.go b/main.go\\nindex...</td>\n",
       "      <td>Fix GetUserName to handle nil user case</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example 2: Simple Bug Fix"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T17:08:29.518520Z",
     "start_time": "2025-01-28T17:08:29.430181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "changes = fetch_git_changes(ROOT / \"data/demos/02-simple-bug-fix\")\n",
    "changes"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'change_type': 'MODIFY',\n",
       "  'old_path': 'main.go',\n",
       "  'new_path': 'main.go',\n",
       "  'diff': 'diff --git a/main.go b/main.go\\nindex 4f59ffd..e19e180 100644\\n--- a/main.go\\n+++ b/main.go\\n@@ -7,6 +7,9 @@ type User struct {\\n}\\nfunc GetUserName(user *User) string {\\n+ if user == nil {\\n+ return \"Unknown\"\\n+ }\\nreturn user.Name\\n}\\n'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Our Model: T5 Efficient Extra Tiny"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T17:08:34.212041Z",
     "start_time": "2025-01-28T17:08:33.960630Z"
    }
   },
   "cell_type": "code",
   "source": "run_inference(t5_efficient_extra_tiny_module, t5_efficient_extra_tiny_datamodule, changes, device)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               input  \\\n",
       "0  main.go diff --git a/main.go b/main.go index 4...   \n",
       "\n",
       "                     prediction  \n",
       "0  Add nil check to User struct  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main.go diff --git a/main.go b/main.go index 4...</td>\n",
       "      <td>Add nil check to User struct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### CMG CodeT5 Model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T17:09:29.906226Z",
     "start_time": "2025-01-28T17:09:29.532056Z"
    }
   },
   "cell_type": "code",
   "source": "run_inference(codet5_module, codet5_datamodule, changes, device)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               input  \\\n",
       "0  main.go\\ndiff --git a/main.go b/main.go\\nindex...   \n",
       "\n",
       "                     prediction  \n",
       "0  Add nil check to GetUserName  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main.go\\ndiff --git a/main.go b/main.go\\nindex...</td>\n",
       "      <td>Add nil check to GetUserName</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### DeepSeek V3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T17:10:26.532396Z",
     "start_time": "2025-01-28T17:10:23.453050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = run_inference_llm(deepseek_v3_module, deepseek_v3_datamodule, changes)\n",
    "display(output)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                      system_content  \\\n",
       "0  You are a helpful assistant that generates com...   \n",
       "\n",
       "                                        user_content  \\\n",
       "0  Code changes:\\nmain.go\\ndiff --git a/main.go b...   \n",
       "\n",
       "                                                diff  \\\n",
       "0  main.go\\ndiff --git a/main.go b/main.go\\nindex...   \n",
       "\n",
       "                                prediction  \n",
       "0  Fix GetUserName to handle nil user case  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_content</th>\n",
       "      <th>user_content</th>\n",
       "      <th>diff</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are a helpful assistant that generates com...</td>\n",
       "      <td>Code changes:\\nmain.go\\ndiff --git a/main.go b...</td>\n",
       "      <td>main.go\\ndiff --git a/main.go b/main.go\\nindex...</td>\n",
       "      <td>Fix GetUserName to handle nil user case</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example 3: New Feature"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T17:24:28.949379Z",
     "start_time": "2025-01-28T17:24:28.815837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "changes = fetch_git_changes(ROOT / \"data/demos/03-new-feature\")\n",
    "changes"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'change_type': 'MODIFY',\n",
       "  'old_path': 'main.go',\n",
       "  'new_path': 'main.go',\n",
       "  'diff': 'diff --git a/main.go b/main.go\\nindex 059a52c..cafd97b 100644\\n--- a/main.go\\n+++ b/main.go\\n@@ -8,4 +8,11 @@ import (\\nfunc main() {\\nu := user.User{Name: \"Alice\", Email: \"alice@example.com\"}\\nfmt.Println(u.GetDetails())\\n+\\n+ // Simulate login attempts\\n+ u.IncrementLoginAttempts()\\n+ u.IncrementLoginAttempts()\\n+\\n+ fmt.Println(\"After login attempts:\")\\n+ fmt.Println(u.GetDetails())\\n}\\n'},\n",
       " {'change_type': 'MODIFY',\n",
       "  'old_path': 'user.go',\n",
       "  'new_path': 'user.go',\n",
       "  'diff': 'diff --git a/user.go b/user.go\\nindex f8d3410..b6d881e 100644\\n--- a/user.go\\n+++ b/user.go\\n@@ -5,8 +5,13 @@ import \"fmt\"\\ntype User struct {\\nName string\\nEmail string\\n+ LoginAttempts int\\n}\\nfunc (u *User) GetDetails() string {\\n- return fmt.Sprintf(\"Name: %s, Email: %s\", u.Name, u.Email)\\n+ return fmt.Sprintf(\"Name: %s, Email: %s, Login Attempts: %d\", u.Name, u.Email, u.LoginAttempts)\\n+}\\n+\\n+func (u *User) IncrementLoginAttempts() {\\n+ u.LoginAttempts++\\n}\\n'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Our Model: T5 Efficient Extra Tiny"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T17:25:10.931464Z",
     "start_time": "2025-01-28T17:25:10.656195Z"
    }
   },
   "cell_type": "code",
   "source": "run_inference(t5_efficient_extra_tiny_module, t5_efficient_extra_tiny_datamodule, changes, device)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               input  \\\n",
       "0  main.go diff --git a/main.go b/main.go index 0...   \n",
       "\n",
       "                      prediction  \n",
       "0  Add login attempts to user.go  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main.go diff --git a/main.go b/main.go index 0...</td>\n",
       "      <td>Add login attempts to user.go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### CMG CodeT5 Model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T17:28:23.568855Z",
     "start_time": "2025-01-28T17:28:23.215860Z"
    }
   },
   "cell_type": "code",
   "source": "run_inference(codet5_module, codet5_datamodule, changes, device)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               input               prediction\n",
       "0  main.go\\ndiff --git a/main.go b/main.go\\nindex...  simulate login attempts"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main.go\\ndiff --git a/main.go b/main.go\\nindex...</td>\n",
       "      <td>simulate login attempts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### DeepSeek v3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T17:28:58.372942Z",
     "start_time": "2025-01-28T17:28:56.024877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = run_inference_llm(deepseek_v3_module, deepseek_v3_datamodule, changes)\n",
    "display(output)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                      system_content  \\\n",
       "0  You are a helpful assistant that generates com...   \n",
       "\n",
       "                                        user_content  \\\n",
       "0  Code changes:\\nmain.go\\ndiff --git a/main.go b...   \n",
       "\n",
       "                                                diff  \\\n",
       "0  main.go\\ndiff --git a/main.go b/main.go\\nindex...   \n",
       "\n",
       "                                          prediction  \n",
       "0  Add LoginAttempts field and IncrementLoginAtte...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_content</th>\n",
       "      <th>user_content</th>\n",
       "      <th>diff</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are a helpful assistant that generates com...</td>\n",
       "      <td>Code changes:\\nmain.go\\ndiff --git a/main.go b...</td>\n",
       "      <td>main.go\\ndiff --git a/main.go b/main.go\\nindex...</td>\n",
       "      <td>Add LoginAttempts field and IncrementLoginAtte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Evaluation Framework\n",
    "\n",
    "The notebook implements two key evaluation functions:\n",
    "- `generate_baseline_message()`: Generates commit messages using the baseline CodeT5 model\n",
    "- `evaluate_messages()`: Uses GPT-4 to evaluate the quality of generated messages compared to target messages on a scale of 1-10\n",
    "\n",
    "The evaluation considers:\n",
    "- The input code diff\n",
    "- Messages from both models\n",
    "- The target (actual) commit message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import rootutils\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "ROOT = rootutils.setup_root(\".\", \".project-root\", pythonpath=True)\n",
    "\n",
    "from src.demo_inference import load_run\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = (\n",
    "    ROOT / \"logs/train/runs/2025-01-24_23-12-54/checkpoints/epoch_023-val_MRR_top5_0.6524.ckpt\"\n",
    ")\n",
    "our_model, datamodule = load_run(checkpoint_path)\n",
    "\n",
    "baseline_tokenizer = AutoTokenizer.from_pretrained(\"JetBrains-Research/cmg-codet5-without-history\")\n",
    "baseline_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"JetBrains-Research/cmg-codet5-without-history\"\n",
    ")\n",
    "baseline_model = baseline_model.to(device)\n",
    "\n",
    "csv_path = ROOT / \"notebooks/comparisons.csv\"\n",
    "samples = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "openai_client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "def generate_baseline_message(diff: str) -> str:\n",
    "    \"\"\"Generate commit message using the baseline model.\"\"\"\n",
    "    inputs = baseline_tokenizer(diff, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = baseline_model.generate(**inputs)\n",
    "    return baseline_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def evaluate_messages(baseline_message: str, target_message: str, diff: str) -> dict:\n",
    "    \"\"\"Evaluate messages using OpenAI.\"\"\"\n",
    "    prompt = f\"\"\"Given a code diff and two commit messages (one from a model and one target message), \n",
    "    evaluate the model message on a scale of 1-10 based on how well it captures the essence of the target message\n",
    "    while maintaining clarity and relevance to the changes.\n",
    "\n",
    "    Code diff:\n",
    "    {diff}\n",
    "\n",
    "    Model Message: {baseline_message}\n",
    "    Target Message: {target_message}\n",
    "\n",
    "    Provide your response in JSON format:\n",
    "    {{\n",
    "        \"score\": <score>,\n",
    "        \"explanation\": \"<brief explanation of the score>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        response_format={\"type\": \"json\"},\n",
    "    )\n",
    "\n",
    "    return json.loads(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Statistical Analysis\n",
    "\n",
    "The notebook performs comprehensive statistical analysis of the evaluation results, including:\n",
    "- Basic descriptive statistics for both models' scores\n",
    "- Analysis based on diff length (categorized into Very Short to Very Long)\n",
    "- Score distributions and comparisons\n",
    "- Correlation between diff length and model performance\n",
    "- Win rate analysis showing the percentage of cases where each model performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"evaluation_results.csv\")\n",
    "\n",
    "basic_stats = pd.DataFrame(\n",
    "    {\n",
    "        \"Tiny Model\": results_df[\"tiny_score\"].describe(),\n",
    "        \"Baseline Model\": results_df[\"baseline_score\"].describe(),\n",
    "        \"Score Difference\": results_df[\"score_difference\"].describe(),\n",
    "    }\n",
    ")\n",
    "print(\"Basic Statistics:\")\n",
    "print(basic_stats)\n",
    "\n",
    "results_df[\"diff_length\"] = results_df[\"input\"].str.len()\n",
    "\n",
    "results_df[\"length_bin\"] = pd.qcut(\n",
    "    results_df[\"diff_length\"], q=5, labels=[\"Very Short\", \"Short\", \"Medium\", \"Long\", \"Very Long\"]\n",
    ")\n",
    "\n",
    "length_stats = (\n",
    "    results_df.groupby(\"length_bin\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"tiny_score\": [\"mean\", \"std\", \"count\"],\n",
    "            \"baseline_score\": [\"mean\", \"std\", \"count\"],\n",
    "            \"score_difference\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "    .round(3)\n",
    ")\n",
    "print(\"\\nScores by Diff Length:\")\n",
    "print(length_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualization\n",
    "## a. Results\n",
    "The results are visualized through multiple plots:\n",
    "- Histograms showing score distributions for both models\n",
    "- A histogram showing the distribution of score differences\n",
    "- Box plots comparing score distributions between models\n",
    "These visualizations help understand the relative performance of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(data=results_df, x=\"tiny_score\", bins=10)\n",
    "plt.title(\"Tiny Model Score Distribution\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(data=results_df, x=\"baseline_score\", bins=10)\n",
    "plt.title(\"Baseline Model Score Distribution\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(data=results_df, x=\"score_difference\", bins=10)\n",
    "plt.title(\"Score Difference Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scores_melted = pd.melt(\n",
    "    results_df[[\"tiny_score\", \"baseline_score\"]], var_name=\"Model\", value_name=\"Score\"\n",
    ")\n",
    "sns.boxplot(data=scores_melted, x=\"Model\", y=\"Score\")\n",
    "plt.title(\"Score Distribution Comparison\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorrelation between diff length and scores:\")\n",
    "correlations = pd.DataFrame(\n",
    "    {\n",
    "        \"Tiny Model\": results_df[\"diff_length\"].corr(results_df[\"tiny_score\"]),\n",
    "        \"Baseline Model\": results_df[\"diff_length\"].corr(results_df[\"baseline_score\"]),\n",
    "    },\n",
    "    index=[\"Correlation with diff length\"],\n",
    ")\n",
    "print(correlations)\n",
    "\n",
    "win_stats = {\n",
    "    \"Tiny Wins\": (results_df[\"score_difference\"] > 0).mean() * 100,\n",
    "    \"Baseline Wins\": (results_df[\"score_difference\"] < 0).mean() * 100,\n",
    "    \"Ties\": (results_df[\"score_difference\"] == 0).mean() * 100,\n",
    "}\n",
    "print(\"\\nWin Rate Analysis (%):\")\n",
    "print(pd.Series(win_stats).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Metrics\n",
    "The notebook calculates and visualizes various metrics for the generated commit messages:\n",
    "- BLEU, ROUGE, BERTScore, and METEOR scores for both models\n",
    "- Visualization of score distributions for each metric\n",
    "- Comparison of average metrics between models\n",
    "These metrics provide a comprehensive evaluation of the generated commit messages.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "from nltk.translate import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "bert_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
    "model = AutoModel.from_pretrained(bert_name)\n",
    "\n",
    "\n",
    "def calculate_metrics(generated_messages, reference_messages):\n",
    "    \"\"\"Calculate ROUGE, BLEU, and METEOR metrics.\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "    metrics = {\"bleu\": [], \"rouge1\": [], \"rouge2\": [], \"rougeL\": [], \"meteor\": []}\n",
    "\n",
    "    for gen, ref in tqdm(\n",
    "        zip(generated_messages, reference_messages), total=len(generated_messages)\n",
    "    ):\n",
    "        # BLEU\n",
    "        bleu = sacrebleu.corpus_bleu([gen], [[ref]], lowercase=True, tokenize=\"13a\").score\n",
    "        metrics[\"bleu\"].append(bleu)\n",
    "\n",
    "        # ROUGE scores\n",
    "        rouge_scores = scorer.score(gen, ref)\n",
    "        metrics[\"rouge1\"].append(rouge_scores[\"rouge1\"].fmeasure)\n",
    "        metrics[\"rouge2\"].append(rouge_scores[\"rouge2\"].fmeasure)\n",
    "        metrics[\"rougeL\"].append(rouge_scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "        # METEOR\n",
    "        meteor = meteor_score.meteor_score([tokenizer.tokenize(ref)], tokenizer.tokenize(gen))\n",
    "        metrics[\"meteor\"].append(meteor)\n",
    "\n",
    "    return {k: np.mean(v) for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "def plot_metrics():\n",
    "    df = pd.read_csv(\"comparisons.csv\")\n",
    "\n",
    "    print(\"Calculating metrics...\")\n",
    "    tiny_metrics = calculate_metrics(df[\"t5-efficient-extra-tiny\"], df[\"target\"])\n",
    "    baseline_metrics = calculate_metrics(df[\"baseline-cmg-codet5-without-history\"], df[\"target\"])\n",
    "\n",
    "    metrics_df = pd.DataFrame({\"Tiny\": tiny_metrics, \"Baseline\": baseline_metrics})\n",
    "\n",
    "    colors = {\"Tiny\": \"#2ecc71\", \"Baseline\": \"#3498db\"}\n",
    "\n",
    "    for metric in metrics_df.index:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        data = [metrics_df.loc[metric, \"Tiny\"], metrics_df.loc[metric, \"Baseline\"]]\n",
    "        bars = plt.bar(\n",
    "            [\"Tiny\", \"Baseline\"],\n",
    "            data,\n",
    "            color=[colors[\"Tiny\"], colors[\"Baseline\"]],\n",
    "            alpha=0.8,\n",
    "            width=0.6,\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{metric} Score Comparison\", pad=20, fontsize=14, fontweight=\"bold\")\n",
    "        plt.ylabel(\"Score\", fontsize=12)\n",
    "        plt.ylim(0, max(data) * 1.2)\n",
    "\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height,\n",
    "                f\"{height:.4f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=11,\n",
    "            )\n",
    "\n",
    "        plt.gca().spines[\"top\"].set_visible(False)\n",
    "        plt.gca().spines[\"right\"].set_visible(False)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\nMetric Summary:\")\n",
    "    print(metrics_df.to_string(float_format=lambda x: \"{:.4f}\".format(x)))\n",
    "\n",
    "\n",
    "plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sample Evaluation\n",
    "The notebook includes a practical demonstration using a sample diff:\n",
    "- Generates commit messages using both models\n",
    "- Shows the target (actual) commit message\n",
    "- Provides a detailed evaluation comparing both generated messages\n",
    "This gives a concrete example of how the models perform in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.demo_inference import generate_commit_message\n",
    "from src.evaluate_commits import CommitMessageEvaluator\n",
    "\n",
    "evaluator = CommitMessageEvaluator(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "sample = samples.iloc[0]\n",
    "print(\"Sample 1 Diff:\\n\", sample[\"input\"][:200] + \"...\\n\")\n",
    "\n",
    "our_message = generate_commit_message(our_model, sample[\"input\"])\n",
    "baseline_message = generate_baseline_message(sample[\"input\"])\n",
    "\n",
    "print(\"Our Model's Message:\", our_message)\n",
    "print(\"Baseline Message:\", baseline_message)\n",
    "print(\"Target Message:\", sample[\"target\"])\n",
    "\n",
    "evaluation = evaluator.evaluate_messages(\n",
    "    tiny_message=our_message,\n",
    "    baseline_message=baseline_message,\n",
    "    target_message=sample[\"target\"],\n",
    "    diff=sample[\"input\"],\n",
    ")\n",
    "print(\"\\nEvaluation:\", json.dumps(evaluation, indent=2))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
