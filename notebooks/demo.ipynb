{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Batch API result processing\n",
    "The notebook includes a practical demonstration using a sample diff:\n",
    "- Generates commit messages using both models\n",
    "- Shows the target (actual) commit message\n",
    "- Provides a detailed evaluation comparing both generated messages\n",
    "This gives a concrete example of how the models perform in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import json\n",
    "#  import pandas as pd\n",
    "\n",
    "#  df = pd.read_csv(\"comparisons.csv\")\n",
    "\n",
    "#  tiny_scores = []\n",
    "#  baseline_scores = []\n",
    "\n",
    "#  with open(\"evaluation_output_500.jsonl\", \"r\") as f:\n",
    "#      for line in f:\n",
    "#          evaluation = json.loads(line)\n",
    "#          # Extract scores from the response\n",
    "#          response_content = json.loads(evaluation[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"])\n",
    "#          tiny_scores.append(response_content[\"tiny_score\"])\n",
    "#          baseline_scores.append(response_content[\"baseline_score\"])\n",
    "\n",
    "#  result_df = df.head(500).copy()\n",
    "#  result_df[\"tiny_score\"] = tiny_scores\n",
    "#  result_df[\"baseline_score\"] = baseline_scores\n",
    "\n",
    "#  result_df[\"score_difference\"] = result_df[\"tiny_score\"] - result_df[\"baseline_score\"]\n",
    "\n",
    "#  result_df.to_csv(\"evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup and Model Loading\n",
    "\n",
    "This notebook demonstrates a comparison between two commit message generation models:\n",
    "- Our fine-tuned model (loaded from a specific checkpoint)\n",
    "- A baseline model (JetBrains-Research/cmg-codet5-without-history)\n",
    "\n",
    "The setup includes importing necessary libraries and configuring the environment to use either CPU or GPU (CUDA) depending on availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import rootutils\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "ROOT = rootutils.setup_root(\".\", \".project-root\", pythonpath=True)\n",
    "\n",
    "from src.demo_inference import load_run\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = (\n",
    "    ROOT / \"logs/train/runs/2025-01-24_23-12-54/checkpoints/epoch_023-val_MRR_top5_0.6524.ckpt\"\n",
    ")\n",
    "our_model, datamodule = load_run(checkpoint_path)\n",
    "\n",
    "baseline_tokenizer = AutoTokenizer.from_pretrained(\"JetBrains-Research/cmg-codet5-without-history\")\n",
    "baseline_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"JetBrains-Research/cmg-codet5-without-history\"\n",
    ")\n",
    "baseline_model = baseline_model.to(device)\n",
    "\n",
    "csv_path = ROOT / \"notebooks/comparisons.csv\"\n",
    "samples = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "openai_client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Evaluation Framework\n",
    "\n",
    "The notebook implements two key evaluation functions:\n",
    "- `generate_baseline_message()`: Generates commit messages using the baseline CodeT5 model\n",
    "- `evaluate_messages()`: Uses GPT-4 to evaluate the quality of generated messages compared to target messages on a scale of 1-10\n",
    "\n",
    "The evaluation considers:\n",
    "- The input code diff\n",
    "- Messages from both models\n",
    "- The target (actual) commit message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_baseline_message(diff: str) -> str:\n",
    "    \"\"\"Generate commit message using the baseline model.\"\"\"\n",
    "    inputs = baseline_tokenizer(diff, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = baseline_model.generate(**inputs)\n",
    "    return baseline_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def evaluate_messages(baseline_message: str, target_message: str, diff: str) -> dict:\n",
    "    \"\"\"Evaluate messages using OpenAI.\"\"\"\n",
    "    prompt = f\"\"\"Given a code diff and two commit messages (one from a model and one target message), \n",
    "    evaluate the model message on a scale of 1-10 based on how well it captures the essence of the target message\n",
    "    while maintaining clarity and relevance to the changes.\n",
    "\n",
    "    Code diff:\n",
    "    {diff}\n",
    "\n",
    "    Model Message: {baseline_message}\n",
    "    Target Message: {target_message}\n",
    "\n",
    "    Provide your response in JSON format:\n",
    "    {{\n",
    "        \"score\": <score>,\n",
    "        \"explanation\": \"<brief explanation of the score>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        response_format={\"type\": \"json\"},\n",
    "    )\n",
    "\n",
    "    return json.loads(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Statistical Analysis\n",
    "\n",
    "The notebook performs comprehensive statistical analysis of the evaluation results, including:\n",
    "- Basic descriptive statistics for both models' scores\n",
    "- Analysis based on diff length (categorized into Very Short to Very Long)\n",
    "- Score distributions and comparisons\n",
    "- Correlation between diff length and model performance\n",
    "- Win rate analysis showing the percentage of cases where each model performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"evaluation_results.csv\")\n",
    "\n",
    "basic_stats = pd.DataFrame(\n",
    "    {\n",
    "        \"Tiny Model\": results_df[\"tiny_score\"].describe(),\n",
    "        \"Baseline Model\": results_df[\"baseline_score\"].describe(),\n",
    "        \"Score Difference\": results_df[\"score_difference\"].describe(),\n",
    "    }\n",
    ")\n",
    "print(\"Basic Statistics:\")\n",
    "print(basic_stats)\n",
    "\n",
    "results_df[\"diff_length\"] = results_df[\"input\"].str.len()\n",
    "\n",
    "results_df[\"length_bin\"] = pd.qcut(\n",
    "    results_df[\"diff_length\"], q=5, labels=[\"Very Short\", \"Short\", \"Medium\", \"Long\", \"Very Long\"]\n",
    ")\n",
    "\n",
    "length_stats = (\n",
    "    results_df.groupby(\"length_bin\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"tiny_score\": [\"mean\", \"std\", \"count\"],\n",
    "            \"baseline_score\": [\"mean\", \"std\", \"count\"],\n",
    "            \"score_difference\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "    .round(3)\n",
    ")\n",
    "print(\"\\nScores by Diff Length:\")\n",
    "print(length_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualization\n",
    "## a. Results\n",
    "The results are visualized through multiple plots:\n",
    "- Histograms showing score distributions for both models\n",
    "- A histogram showing the distribution of score differences\n",
    "- Box plots comparing score distributions between models\n",
    "These visualizations help understand the relative performance of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(data=results_df, x=\"tiny_score\", bins=10)\n",
    "plt.title(\"Tiny Model Score Distribution\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(data=results_df, x=\"baseline_score\", bins=10)\n",
    "plt.title(\"Baseline Model Score Distribution\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(data=results_df, x=\"score_difference\", bins=10)\n",
    "plt.title(\"Score Difference Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scores_melted = pd.melt(\n",
    "    results_df[[\"tiny_score\", \"baseline_score\"]], var_name=\"Model\", value_name=\"Score\"\n",
    ")\n",
    "sns.boxplot(data=scores_melted, x=\"Model\", y=\"Score\")\n",
    "plt.title(\"Score Distribution Comparison\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorrelation between diff length and scores:\")\n",
    "correlations = pd.DataFrame(\n",
    "    {\n",
    "        \"Tiny Model\": results_df[\"diff_length\"].corr(results_df[\"tiny_score\"]),\n",
    "        \"Baseline Model\": results_df[\"diff_length\"].corr(results_df[\"baseline_score\"]),\n",
    "    },\n",
    "    index=[\"Correlation with diff length\"],\n",
    ")\n",
    "print(correlations)\n",
    "\n",
    "win_stats = {\n",
    "    \"Tiny Wins\": (results_df[\"score_difference\"] > 0).mean() * 100,\n",
    "    \"Baseline Wins\": (results_df[\"score_difference\"] < 0).mean() * 100,\n",
    "    \"Ties\": (results_df[\"score_difference\"] == 0).mean() * 100,\n",
    "}\n",
    "print(\"\\nWin Rate Analysis (%):\")\n",
    "print(pd.Series(win_stats).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Metrics\n",
    "The notebook calculates and visualizes various metrics for the generated commit messages:\n",
    "- BLEU, ROUGE, BERTScore, and METEOR scores for both models\n",
    "- Visualization of score distributions for each metric\n",
    "- Comparison of average metrics between models\n",
    "These metrics provide a comprehensive evaluation of the generated commit messages.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "from nltk.translate import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "bert_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
    "model = AutoModel.from_pretrained(bert_name)\n",
    "\n",
    "\n",
    "def calculate_metrics(generated_messages, reference_messages):\n",
    "    \"\"\"Calculate ROUGE, BLEU, and METEOR metrics.\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "    metrics = {\"bleu\": [], \"rouge1\": [], \"rouge2\": [], \"rougeL\": [], \"meteor\": []}\n",
    "\n",
    "    for gen, ref in tqdm(\n",
    "        zip(generated_messages, reference_messages), total=len(generated_messages)\n",
    "    ):\n",
    "        # BLEU\n",
    "        bleu = sacrebleu.corpus_bleu([gen], [[ref]], lowercase=True, tokenize=\"13a\").score\n",
    "        metrics[\"bleu\"].append(bleu)\n",
    "\n",
    "        # ROUGE scores\n",
    "        rouge_scores = scorer.score(gen, ref)\n",
    "        metrics[\"rouge1\"].append(rouge_scores[\"rouge1\"].fmeasure)\n",
    "        metrics[\"rouge2\"].append(rouge_scores[\"rouge2\"].fmeasure)\n",
    "        metrics[\"rougeL\"].append(rouge_scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "        # METEOR\n",
    "        meteor = meteor_score.meteor_score([tokenizer.tokenize(ref)], tokenizer.tokenize(gen))\n",
    "        metrics[\"meteor\"].append(meteor)\n",
    "\n",
    "    return {k: np.mean(v) for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "def plot_metrics():\n",
    "    df = pd.read_csv(\"comparisons.csv\")\n",
    "\n",
    "    print(\"Calculating metrics...\")\n",
    "    tiny_metrics = calculate_metrics(df[\"t5-efficient-extra-tiny\"], df[\"target\"])\n",
    "    baseline_metrics = calculate_metrics(df[\"baseline-cmg-codet5-without-history\"], df[\"target\"])\n",
    "\n",
    "    metrics_df = pd.DataFrame({\"Tiny\": tiny_metrics, \"Baseline\": baseline_metrics})\n",
    "\n",
    "    colors = {\"Tiny\": \"#2ecc71\", \"Baseline\": \"#3498db\"}\n",
    "\n",
    "    for metric in metrics_df.index:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        data = [metrics_df.loc[metric, \"Tiny\"], metrics_df.loc[metric, \"Baseline\"]]\n",
    "        bars = plt.bar(\n",
    "            [\"Tiny\", \"Baseline\"],\n",
    "            data,\n",
    "            color=[colors[\"Tiny\"], colors[\"Baseline\"]],\n",
    "            alpha=0.8,\n",
    "            width=0.6,\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{metric} Score Comparison\", pad=20, fontsize=14, fontweight=\"bold\")\n",
    "        plt.ylabel(\"Score\", fontsize=12)\n",
    "        plt.ylim(0, max(data) * 1.2)\n",
    "\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height,\n",
    "                f\"{height:.4f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=11,\n",
    "            )\n",
    "\n",
    "        plt.gca().spines[\"top\"].set_visible(False)\n",
    "        plt.gca().spines[\"right\"].set_visible(False)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\nMetric Summary:\")\n",
    "    print(metrics_df.to_string(float_format=lambda x: \"{:.4f}\".format(x)))\n",
    "\n",
    "\n",
    "plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sample Evaluation\n",
    "The notebook includes a practical demonstration using a sample diff:\n",
    "- Generates commit messages using both models\n",
    "- Shows the target (actual) commit message\n",
    "- Provides a detailed evaluation comparing both generated messages\n",
    "This gives a concrete example of how the models perform in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.demo_inference import generate_commit_message\n",
    "from src.evaluate_commits import CommitMessageEvaluator\n",
    "\n",
    "evaluator = CommitMessageEvaluator(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "sample = samples.iloc[0]\n",
    "print(\"Sample 1 Diff:\\n\", sample[\"input\"][:200] + \"...\\n\")\n",
    "\n",
    "our_message = generate_commit_message(our_model, sample[\"input\"])\n",
    "baseline_message = generate_baseline_message(sample[\"input\"])\n",
    "\n",
    "print(\"Our Model's Message:\", our_message)\n",
    "print(\"Baseline Message:\", baseline_message)\n",
    "print(\"Target Message:\", sample[\"target\"])\n",
    "\n",
    "evaluation = evaluator.evaluate_messages(\n",
    "    tiny_message=our_message,\n",
    "    baseline_message=baseline_message,\n",
    "    target_message=sample[\"target\"],\n",
    "    diff=sample[\"input\"],\n",
    ")\n",
    "print(\"\\nEvaluation:\", json.dumps(evaluation, indent=2))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
