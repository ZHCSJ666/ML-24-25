{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a643813c04681d7",
   "metadata": {},
   "source": [
    "# Data Collator Experiments\n",
    "\n",
    "In this notebook, we'll explore how to construct batches out of processed `Commit Chronicle` dataset during the training/validation setting for a encoder-decoder style architecture.\n",
    "\n",
    "**Make sure to run `Commit Chronicle Dataset.ipynb` before using this notebook.**\n",
    "\n",
    "The logic laid out in this notebook is implemented in `DataCollatorTrain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d582a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add the parent of the src directory for the imports to work\n",
    "my_src_path = 'ML-24-25'\n",
    "sys.path.append(my_src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:42.359455Z",
     "start_time": "2024-11-09T07:11:39.362450Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rootutils\n",
    "import torch.utils.data\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from src.data.types import SingleExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8271185ef40822a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:42.365553Z",
     "start_time": "2024-11-09T07:11:42.361964Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT = rootutils.find_root(\".\", \".project-root\")\n",
    "OUTPUT_DIR = ROOT / \"data/playground\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b66560668fa852eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:42.739422Z",
     "start_time": "2024-11-09T07:11:42.569554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['author', 'msg_input_ids', 'diff_input_ids', 'language', 'repo'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ = load_from_disk(OUTPUT_DIR / \"02-processed\")\n",
    "dataset_.select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21977b087089398c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:42.777409Z",
     "start_time": "2024-11-09T07:11:42.773621Z"
    }
   },
   "outputs": [],
   "source": [
    "class HumbleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index: int) -> SingleExample:\n",
    "        row = self.dataset[index]\n",
    "        return SingleExample(\n",
    "            diff_input_ids=row[\"diff_input_ids\"],\n",
    "            msg_input_ids=row[\"msg_input_ids\"],\n",
    "            history_input_ids=[],  # ignored in this notebook. don't worry out it. trust me :)\n",
    "            pos_in_file=-1,  # ignored in this notebook.\n",
    "        )\n",
    "\n",
    "\n",
    "data = HumbleDataset(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d18fd5dede64a7c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:44.009361Z",
     "start_time": "2024-11-09T07:11:42.826058Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.data.components.tokenization import add_special_tokens\n",
    "from transformers import AutoTokenizer\n",
    "from copy import deepcopy\n",
    "\n",
    "msg_tokenizer_ = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
    "msg_tokenizer_ = add_special_tokens(msg_tokenizer_, None)\n",
    "diff_tokenizer_ = deepcopy(msg_tokenizer_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab645ac76704b58",
   "metadata": {},
   "source": [
    "# Encoder Input Processing\n",
    "\n",
    "Here we assume input to the encoder is the git diff, `diff_input_ids` attribute of `SingleExample`. It can also be history of all git diffs, but we don't use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5347e3cf34422c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:44.018397Z",
     "start_time": "2024-11-09T07:11:44.012358Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_encoder_inputs(\n",
    "    input_ids_batch: list[list[int]],\n",
    "    encoder_context_max_len: int,\n",
    "    bos_token_id: int,\n",
    "    eos_token_id: int,\n",
    "    pad_token_id: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This helper method processes either diffs or messages as encoder input.\n",
    "\n",
    "    It truncates the inputs to the maximum allowed length.\n",
    "\n",
    "    It also adds all required special tokens: format is [BOS] input [EOS].\n",
    "\n",
    "    Finally, it is responsible for padding to maximum length in batch and conversion to torch.Tensor.\n",
    "\n",
    "    Args:\n",
    "        input_ids_batch: A list of tokenized examples from the current batch.\n",
    "        encoder_context_max_len: The maximum length of the encoder context.\n",
    "        bos_token_id: The value of the beginning of sequence (BOS) token.\n",
    "        eos_token_id: The value of the end of sequence (EOS) token.\n",
    "        pad_token_id: The value of the padding token (PAD) token.\n",
    "\n",
    "    Returns:\n",
    "        input_ids for encoder, attention_mask for encoder\n",
    "    \"\"\"\n",
    "\n",
    "    # add BOS and EOS tokens to each example whilst making sure max length of resulting token list is encoder_context_max_len\n",
    "    input_ids_batch = [\n",
    "        [bos_token_id] + example[: encoder_context_max_len - 2] + [eos_token_id]\n",
    "        for example in input_ids_batch\n",
    "    ]\n",
    "    inputs_tensors = [torch.tensor(ids, dtype=torch.int64) for ids in input_ids_batch]\n",
    "\n",
    "    # pad tensors to max length in batch\n",
    "    inputs_max_len = max(len(tensor) for tensor in input_ids_batch)\n",
    "    inputs_tensors = [\n",
    "        _pad_tensor(\n",
    "            tensor,\n",
    "            pad_len=inputs_max_len - tensor.numel(),\n",
    "            value=pad_token_id,\n",
    "            left=False,\n",
    "        )\n",
    "        for tensor in inputs_tensors\n",
    "    ]\n",
    "\n",
    "    masks_tensors = [torch.ones_like(ids) for ids in inputs_tensors]\n",
    "    masks_tensors = [\n",
    "        _pad_tensor(\n",
    "            tensor,\n",
    "            pad_len=inputs_max_len - tensor.numel(),\n",
    "            value=0,\n",
    "            left=False,\n",
    "        )\n",
    "        for tensor in masks_tensors\n",
    "    ]\n",
    "    return torch.stack(inputs_tensors), torch.stack(masks_tensors)\n",
    "\n",
    "\n",
    "def _pad_tensor(\n",
    "    input_tensor: torch.Tensor, pad_len: int, value: int, left: bool\n",
    ") -> torch.Tensor:\n",
    "    return torch.nn.functional.pad(\n",
    "        input_tensor,\n",
    "        pad=[pad_len, 0] if left else [0, pad_len],\n",
    "        mode=\"constant\",\n",
    "        value=value,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7aa941fcb1555",
   "metadata": {},
   "source": [
    "Let's try it out with a batch size of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc54c9d6da4cdf12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:44.037073Z",
     "start_time": "2024-11-09T07:11:44.031102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 512]), torch.Size([2, 512]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_ = [data[0], data[1]]\n",
    "git_diff_inputs_ = [example.diff_input_ids for example in examples_]\n",
    "encoder_input_ids_, encoder_attention_mask_ = process_encoder_inputs(\n",
    "    input_ids_batch=git_diff_inputs_,\n",
    "    encoder_context_max_len=512,  # this is a hyperparameter\n",
    "    bos_token_id=diff_tokenizer_.bos_token_id,\n",
    "    eos_token_id=diff_tokenizer_.eos_token_id,\n",
    "    pad_token_id=diff_tokenizer_.pad_token_id,\n",
    ")\n",
    "encoder_input_ids_.shape, encoder_attention_mask_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89460b619b8d24fc",
   "metadata": {},
   "source": [
    "That's it. The output data forms input to our encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb51094f1f0c486b",
   "metadata": {},
   "source": [
    "# Decoder Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d1c47d6d0e78d46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:44.063797Z",
     "start_time": "2024-11-09T07:11:44.053431Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "\n",
    "\n",
    "def _process_decoder_input(\n",
    "    examples: list[SingleExample],\n",
    "    msg_bos_token_id: int,\n",
    "    msg_eos_token_id: int,\n",
    "    msg_pad_token_id: int,\n",
    "    decoder_context_max_len,\n",
    "    shift_labels: bool,\n",
    "    decoder_start_token_id: Optional[int] = None,\n",
    "    # ignore these options\n",
    "    encoder_input_type: Literal[\"diff\", \"history\"] = \"diff\",\n",
    "    with_history: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepares decoder input for train/validation:\n",
    "      * aggregates messages from history when configured accordingly\n",
    "      * concatenates history with current message\n",
    "      * constructs labels\n",
    "      * pads, converts to tensors\n",
    "\n",
    "    Args:\n",
    "        examples: A list of inputs for current batch.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of three tensors: input ids, attention masks, labels.\n",
    "    \"\"\"\n",
    "    message_inputs: list[list[int]] = [example.msg_input_ids for example in examples]\n",
    "    history_inputs: list[list[list[int]]] = [\n",
    "        example.history_input_ids for example in examples\n",
    "    ]\n",
    "\n",
    "    all_msg_ids: list[torch.Tensor] = []\n",
    "    all_msg_masks: list[torch.Tensor] = []\n",
    "    all_msg_labels: list[torch.Tensor] = []\n",
    "\n",
    "    for message_ids, history_ids in zip(message_inputs, history_inputs):\n",
    "        message_ids = message_ids[: decoder_context_max_len - 2]\n",
    "\n",
    "        cur_history_ids = []\n",
    "        cur_history_labels = []\n",
    "\n",
    "        # if encoder_input_type != \"history\" and with_history:\n",
    "        #     cur_history_ids = _get_history(\n",
    "        #         cur_len=len(message_ids) + 2,\n",
    "        #         history_ids=history_ids,\n",
    "        #     )\n",
    "        #     cur_history_labels = [\n",
    "        #         [-100 for _ in message] for message in cur_history_ids\n",
    "        #     ]\n",
    "\n",
    "        cur_ids = (\n",
    "            [[msg_bos_token_id]]\n",
    "            + cur_history_ids\n",
    "            + [message_ids]\n",
    "            + [[msg_eos_token_id]]\n",
    "        )\n",
    "        cur_labels = (\n",
    "            [[msg_bos_token_id]]\n",
    "            + cur_history_labels\n",
    "            + [message_ids]\n",
    "            + [[msg_eos_token_id]]\n",
    "        )\n",
    "\n",
    "        if shift_labels:\n",
    "            cur_ids, cur_labels = _shift_for_encoder_decoder(\n",
    "                cur_ids,\n",
    "                cur_labels,\n",
    "                msg_bos_token_id=msg_bos_token_id,\n",
    "                decoder_start_token_id=decoder_start_token_id,\n",
    "            )\n",
    "\n",
    "        cur_ids_tensor = torch.tensor(\n",
    "            [ex for sublist in cur_ids for ex in sublist], dtype=torch.int64\n",
    "        )\n",
    "        cur_labels_tensor = torch.tensor(\n",
    "            [ex for sublist in cur_labels for ex in sublist], dtype=torch.int64\n",
    "        )\n",
    "        cur_mask_tensor = torch.ones_like(cur_ids_tensor)\n",
    "\n",
    "        all_msg_ids.append(cur_ids_tensor)\n",
    "        all_msg_masks.append(cur_mask_tensor)\n",
    "        all_msg_labels.append(cur_labels_tensor)\n",
    "\n",
    "    msg_max_len = max(len(tensor) for tensor in all_msg_ids)\n",
    "    all_msg_ids = [\n",
    "        _pad_tensor(\n",
    "            tensor,\n",
    "            pad_len=msg_max_len - tensor.numel(),\n",
    "            value=msg_pad_token_id,\n",
    "            left=False,\n",
    "        )\n",
    "        for tensor in all_msg_ids\n",
    "    ]\n",
    "    all_msg_masks = [\n",
    "        _pad_tensor(\n",
    "            tensor,\n",
    "            pad_len=msg_max_len - tensor.numel(),\n",
    "            value=0,\n",
    "            left=False,\n",
    "        )\n",
    "        for tensor in all_msg_masks\n",
    "    ]\n",
    "    all_msg_labels = [\n",
    "        _pad_tensor(\n",
    "            tensor,\n",
    "            pad_len=msg_max_len - tensor.numel(),\n",
    "            value=-100,\n",
    "            left=False,\n",
    "        )\n",
    "        for tensor in all_msg_labels\n",
    "    ]\n",
    "\n",
    "    return (\n",
    "        torch.stack(all_msg_ids),\n",
    "        torch.stack(all_msg_masks),\n",
    "        torch.stack(all_msg_labels),\n",
    "    )\n",
    "\n",
    "\n",
    "def _shift_for_encoder_decoder(\n",
    "    ids: list[list[int]],\n",
    "    labels: list[list[int]],\n",
    "    msg_bos_token_id: int,\n",
    "    decoder_start_token_id: Optional[int] = None,\n",
    ") -> tuple[list[list[int]], list[list[int]]]:\n",
    "    \"\"\"This method mimics transformers logic of ids and labels for EncoderDecoderModel\n",
    "    (or T5ForConditionalGeneration).\n",
    "\n",
    "    Starting from transformers v4.12, loss is now calculated in EncoderDecoderModel, not in decoder class.\n",
    "    Also, decoder input ids are created automatically based on labels: labels are shifted and -100 is replaced\n",
    "    with pad token. In our case, history ids are masked -100 in labels, but they are still\n",
    "    meaningful ids. Therefore, we can't use the default approach.\n",
    "    \"\"\"\n",
    "    if decoder_start_token_id is None:\n",
    "        ids = [[msg_bos_token_id]] + ids[:-1]\n",
    "    else:\n",
    "        ids = [[decoder_start_token_id]] + ids[:-1]\n",
    "    return ids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d112c8309ce4be49",
   "metadata": {},
   "source": [
    "Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acd125d3780d419c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:44.082015Z",
     "start_time": "2024-11-09T07:11:44.077804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 26]), torch.Size([2, 26]), torch.Size([2, 26]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_ids_, decoder_attention_mask_, labels_ = _process_decoder_input(\n",
    "    examples=examples_,\n",
    "    msg_bos_token_id=msg_tokenizer_.bos_token_id,\n",
    "    msg_eos_token_id=msg_tokenizer_.eos_token_id,\n",
    "    msg_pad_token_id=msg_tokenizer_.pad_token_id,\n",
    "    decoder_context_max_len=512,  # this is a hyperparam for the model\n",
    "    shift_labels=True,\n",
    ")\n",
    "decoder_input_ids_.shape, decoder_attention_mask_.shape, labels_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caff40753de02be",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acff07ec03fcb9b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:46.378208Z",
     "start_time": "2024-11-09T07:11:44.095188Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "719ffd6afe276cbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:46.745202Z",
     "start_time": "2024-11-09T07:11:46.397566Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['clear',\n",
       " 'copy',\n",
       " 'cross_attentions',\n",
       " 'decoder_attentions',\n",
       " 'decoder_hidden_states',\n",
       " 'encoder_attentions',\n",
       " 'encoder_hidden_states',\n",
       " 'encoder_last_hidden_state',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'logits',\n",
       " 'loss',\n",
       " 'move_to_end',\n",
       " 'past_key_values',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'to_tuple',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(\n",
    "    input_ids=encoder_input_ids_,\n",
    "    attention_mask=encoder_attention_mask_,\n",
    "    decoder_input_ids=decoder_input_ids_,\n",
    "    decoder_attention_mask=decoder_attention_mask_,\n",
    "    labels=labels_,\n",
    ")\n",
    "[attr for attr in dir(outputs) if not attr.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ccdd890a07d4527",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:46.951214Z",
     "start_time": "2024-11-09T07:11:46.947128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.9106, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c174b52a4311ef92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:13:39.596713Z",
     "start_time": "2024-11-09T07:11:47.105166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 Loss:14.2112\n",
      "Epoch: 001 Loss:13.3545\n",
      "Epoch: 002 Loss:14.5035\n",
      "Epoch: 003 Loss:13.6910\n",
      "Epoch: 004 Loss:13.5256\n",
      "Epoch: 005 Loss:13.7238\n",
      "Epoch: 006 Loss:14.8889\n",
      "Epoch: 007 Loss:12.7104\n",
      "Epoch: 008 Loss:13.9504\n",
      "Epoch: 009 Loss:13.7313\n",
      "Epoch: 010 Loss:13.5998\n",
      "Epoch: 011 Loss:12.8251\n",
      "Epoch: 012 Loss:15.5966\n",
      "Epoch: 013 Loss:13.9444\n",
      "Epoch: 014 Loss:12.7470\n",
      "Epoch: 015 Loss:13.0972\n",
      "Epoch: 016 Loss:13.1960\n",
      "Epoch: 017 Loss:13.6399\n",
      "Epoch: 018 Loss:13.2541\n",
      "Epoch: 019 Loss:14.0269\n",
      "Epoch: 020 Loss:14.6821\n",
      "Epoch: 021 Loss:13.2320\n",
      "Epoch: 022 Loss:13.8468\n",
      "Epoch: 023 Loss:13.9215\n",
      "Epoch: 024 Loss:14.1133\n",
      "Epoch: 025 Loss:13.8879\n",
      "Epoch: 026 Loss:14.0524\n",
      "Epoch: 027 Loss:13.4823\n",
      "Epoch: 028 Loss:13.3946\n",
      "Epoch: 029 Loss:13.6241\n",
      "Epoch: 030 Loss:12.7660\n",
      "Epoch: 031 Loss:13.5473\n",
      "Epoch: 032 Loss:13.5531\n",
      "Epoch: 033 Loss:15.0437\n",
      "Epoch: 034 Loss:14.7943\n",
      "Epoch: 035 Loss:13.0712\n",
      "Epoch: 036 Loss:12.6115\n",
      "Epoch: 037 Loss:13.6340\n",
      "Epoch: 038 Loss:14.8388\n",
      "Epoch: 039 Loss:12.6437\n",
      "Epoch: 040 Loss:12.6444\n",
      "Epoch: 041 Loss:14.1865\n",
      "Epoch: 042 Loss:12.6814\n",
      "Epoch: 043 Loss:13.7471\n",
      "Epoch: 044 Loss:13.9625\n",
      "Epoch: 045 Loss:14.4251\n",
      "Epoch: 046 Loss:13.6759\n",
      "Epoch: 047 Loss:12.6452\n",
      "Epoch: 048 Loss:14.6907\n",
      "Epoch: 049 Loss:12.2727\n",
      "Epoch: 050 Loss:14.5981\n",
      "Epoch: 051 Loss:13.6316\n",
      "Epoch: 052 Loss:13.8678\n",
      "Epoch: 053 Loss:13.4931\n",
      "Epoch: 054 Loss:13.3954\n",
      "Epoch: 055 Loss:13.7028\n",
      "Epoch: 056 Loss:13.6006\n",
      "Epoch: 057 Loss:12.9613\n",
      "Epoch: 058 Loss:13.7105\n",
      "Epoch: 059 Loss:13.3202\n",
      "Epoch: 060 Loss:14.3676\n",
      "Epoch: 061 Loss:13.4897\n",
      "Epoch: 062 Loss:13.0630\n",
      "Epoch: 063 Loss:13.3501\n",
      "Epoch: 064 Loss:12.8005\n",
      "Epoch: 065 Loss:13.0440\n",
      "Epoch: 066 Loss:13.1486\n",
      "Epoch: 067 Loss:13.1821\n",
      "Epoch: 068 Loss:13.2998\n",
      "Epoch: 069 Loss:14.5460\n",
      "Epoch: 070 Loss:13.3082\n",
      "Epoch: 071 Loss:13.8947\n",
      "Epoch: 072 Loss:12.9957\n",
      "Epoch: 073 Loss:12.6773\n",
      "Epoch: 074 Loss:13.6557\n",
      "Epoch: 075 Loss:14.2226\n",
      "Epoch: 076 Loss:12.9094\n",
      "Epoch: 077 Loss:13.2960\n",
      "Epoch: 078 Loss:13.2613\n",
      "Epoch: 079 Loss:14.7312\n",
      "Epoch: 080 Loss:14.3386\n",
      "Epoch: 081 Loss:13.2413\n",
      "Epoch: 082 Loss:13.0910\n",
      "Epoch: 083 Loss:13.0856\n",
      "Epoch: 084 Loss:13.0472\n",
      "Epoch: 085 Loss:13.3929\n",
      "Epoch: 086 Loss:12.6694\n",
      "Epoch: 087 Loss:14.4647\n",
      "Epoch: 088 Loss:14.7427\n",
      "Epoch: 089 Loss:12.7717\n",
      "Epoch: 090 Loss:13.9318\n",
      "Epoch: 091 Loss:12.2308\n",
      "Epoch: 092 Loss:13.4670\n",
      "Epoch: 093 Loss:14.5572\n",
      "Epoch: 094 Loss:12.2158\n",
      "Epoch: 095 Loss:12.5846\n",
      "Epoch: 096 Loss:13.8933\n",
      "Epoch: 097 Loss:13.7685\n",
      "Epoch: 098 Loss:15.2585\n",
      "Epoch: 099 Loss:13.4798\n"
     ]
    }
   ],
   "source": [
    "# let's overfit\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\").train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=0.1)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(\n",
    "        input_ids=encoder_input_ids_,\n",
    "        attention_mask=encoder_attention_mask_,\n",
    "        decoder_input_ids=decoder_input_ids_,\n",
    "        decoder_attention_mask=decoder_attention_mask_,\n",
    "        labels=labels_,\n",
    "    )\n",
    "\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {i:03d} Loss:{loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e21c29bbf9097",
   "metadata": {},
   "source": [
    "I was expecting the model loss to reduce smoothly but that didn't happen. Hmmmmm..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac3a4768dba834c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:13:39.835864Z",
     "start_time": "2024-11-09T07:13:39.833651Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
