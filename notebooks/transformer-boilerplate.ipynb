{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to generate synthetic commit data\n",
    "Wrote this before we had the commit chronicles dataset, you can disregard this step and jump directly to the next, or you can use it as a preliminary testing area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rootutils\n",
    "import torch.nn as nn\n",
    "from tokenizers import Tokenizer, decoders, models, pre_tokenizers, processors, trainers\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = rootutils.setup_root(\".\", \".project-root\", pythonpath=True)\n",
    "OUTPUT_DIR = ROOT / \"data/playground\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_names = [\n",
    "    \"display_greeting\",\n",
    "    \"show_warning\",\n",
    "    \"print_info\",\n",
    "    \"log_event\",\n",
    "    \"announce_start\",\n",
    "    \"notify_completion\",\n",
    "    \"output_status\",\n",
    "    \"send_alert\",\n",
    "    # \"record_activity\", \"trace_execution\", \"emit_signal\", \"broadcast_message\",\n",
    "    # \"report_error\", \"inform_user\", \"update_display\", \"refresh_view\",\n",
    "    # \"render_output\", \"present_cdata\", \"show_notification\", \"display_result\",\n",
    "    # \"print_summary\", \"log_details\", \"notify_admin\", \"output_log\",\n",
    "    # \"send_update\", \"record_log\", \"trace_process\", \"emit_event\", \"broadcast_alert\",\n",
    "    # \"report_status\", \"inform_system\",\n",
    "    # \"initialize_module\", \"shutdown_service\", \"handle_request\", \"process_data\",\n",
    "    # \"validate_input\", \"authenticate_user\", \"authorize_access\", \"compress_files\",\n",
    "    # \"decompress_files\", \"backup_database\", \"restore_database\", \"sync_files\",\n",
    "    # \"monitor_performance\", \"optimize_queries\", \"generate_report\", \"send_email\",\n",
    "    # \"schedule_task\", \"cancel_task\", \"retry_operation\", \"log_transaction\",\n",
    "    # \"update_configuration\", \"load_settings\", \"parse_response\", \"manage_sessions\",\n",
    "    # \"encrypt_data\", \"decrypt_data\", \"format_output\", \"validate_credentials\",\n",
    "    # \"handle_errors\", \"process_payments\", \"manage_notifications\", \"track_usage\",\n",
    "    # \"generate_token\", \"verify_identity\", \"log_out\", \"register_user\"\n",
    "]\n",
    "\n",
    "printed_messages = [\n",
    "    \"Hello, World!\",\n",
    "    \"Warning: Low Disk Space.\",\n",
    "    \"Information: Process started.\",\n",
    "    \"Event logged successfully.\",\n",
    "    \"Starting the application.\",\n",
    "    \"Completion successful.\",\n",
    "    \"Status: All systems operational.\",\n",
    "    \"Alert: Unauthorized access detected.\",\n",
    "    # \"Activity recorded.\", \"Execution trace started.\", \"Signal emitted.\",\n",
    "    # \"Message broadcasted.\", \"Error encountered in module.\", \"User informed.\",\n",
    "    # \"Display updated.\", \"View refreshed.\", \"Output rendered.\", \"Data presented.\",\n",
    "    # \"Notification displayed.\", \"Result shown.\", \"Summary printed.\",\n",
    "    # \"Details logged.\", \"Admin notified.\", \"Log output generated.\",\n",
    "    # \"Update sent to server.\", \"Log recorded.\", \"Process traced.\", \"Event emitted.\",\n",
    "    # \"Alert broadcasted.\", \"Status reported.\", \"System informed.\",\n",
    "    # \"Module initialized.\", \"Service shutdown gracefully.\", \"Request handled successfully.\",\n",
    "    # \"Data processed without errors.\", \"Input validated.\", \"User authenticated.\",\n",
    "    # \"Access authorized.\", \"Files compressed.\", \"Files decompressed.\",\n",
    "    # \"Database backed up.\", \"Database restored.\", \"Files synchronized.\",\n",
    "    # \"Performance monitored.\", \"Queries optimized.\", \"Report generated.\",\n",
    "    # \"Email sent successfully.\", \"Task scheduled.\", \"Task canceled.\",\n",
    "    # \"Operation retried.\", \"Transaction logged.\", \"Configuration updated.\",\n",
    "    # \"Settings loaded.\", \"Response parsed.\", \"Session managed.\",\n",
    "    # \"Data encrypted.\", \"Data decrypted.\", \"Output formatted.\", \"Credentials validated.\",\n",
    "    # \"Errors handled.\", \"Payments processed.\", \"Notifications managed.\",\n",
    "    # \"Usage tracked.\", \"Token generated.\", \"Identity verified.\", \"User logged out.\",\n",
    "    # \"User registered successfully.\"\n",
    "]\n",
    "\n",
    "file_names = [\n",
    "    \"utils.py\",\n",
    "    \"helpers.py\",\n",
    "    \"main.py\",\n",
    "    \"scripts.py\",\n",
    "    \"commands.py\",\n",
    "    \"logger.py\",\n",
    "    \"notifications.py\",\n",
    "    \"functions.py\",\n",
    "    \"actions.py\",\n",
    "    \"alerts.py\",\n",
    "    \"outputs.py\",\n",
    "    \"interactions.py\",\n",
    "    \"communication.py\",\n",
    "    # \"handlers.py\", \"response.py\", \"initializer.py\", \"setup.py\", \"runner.py\",\n",
    "    # \"manager.py\", \"processor.py\", \"controller.py\", \"service.py\", \"adapter.py\",\n",
    "    # \"connector.py\", \"dispatcher.py\", \"executor.py\", \"facade.py\", \"gateway.py\",\n",
    "    # \"handler.py\", \"integrator.py\", \"mediator.py\", \"observer.py\", \"provider.py\",\n",
    "    # \"registrar.py\", \"scheduler.py\", \"translator.py\", \"validator.py\", \"watcher.py\",\n",
    "    # \"database.py\", \"authentication.py\", \"authorization.py\", \"backup.py\",\n",
    "    # \"monitor.py\", \"reporting.py\", \"email_service.py\", \"task_manager.py\",\n",
    "    # \"transaction.py\", \"compression.py\", \"decompression.py\", \"synchronization.py\",\n",
    "    # \"performance.py\", \"optimization.py\",\n",
    "    # \"config.py\", \"utils_v2.py\", \"helpers_v2.py\", \"main_v2.py\",\n",
    "    # \"scripts_v2.py\", \"commands_v2.py\", \"logger_v2.py\", \"notifications_v2.py\",\n",
    "    # \"functions_v2.py\", \"actions_v2.py\", \"alerts_v2.py\", \"outputs_v2.py\",\n",
    "    # \"interactions_v2.py\", \"communication_v2.py\", \"handlers_v2.py\",\n",
    "    # \"response_v2.py\", \"initializer_v2.py\", \"setup_v2.py\", \"runner_v2.py\",\n",
    "    # \"manager_v2.py\", \"processor_v2.py\", \"controller_v2.py\", \"service_v2.py\",\n",
    "    # \"adapter_v2.py\", \"connector_v2.py\", \"dispatcher_v2.py\", \"executor_v2.py\",\n",
    "    # \"facade_v2.py\", \"gateway_v2.py\", \"handler_v2.py\", \"integrator_v2.py\",\n",
    "    # \"mediator_v2.py\", \"observer_v2.py\", \"provider_v2.py\", \"registrar_v2.py\",\n",
    "    # \"scheduler_v2.py\", \"translator_v2.py\", \"validator_v2.py\", \"watcher_v2.py\",\n",
    "    # \"database_v2.py\", \"authentication_v2.py\", \"authorization_v2.py\",\n",
    "    # \"backup_v2.py\", \"monitor_v2.py\", \"reporting_v2.py\", \"email_service_v2.py\",\n",
    "    # \"task_manager_v2.py\", \"transaction_v2.py\", \"compression_v2.py\",\n",
    "    # \"decompression_v2.py\", \"synchronization_v2.py\", \"performance_v2.py\",\n",
    "    # \"optimization_v2.py\"\n",
    "]\n",
    "\n",
    "\n",
    "def generate_file_content(function_name, message):\n",
    "    \"\"\"\n",
    "    Generates the complete content of a Python file by concatenating\n",
    "    the new function definition.\n",
    "    \"\"\"\n",
    "    function_def = f\"def {function_name}():\\n\" f'    print(\"{message}\")\\n'\n",
    "    return function_def\n",
    "\n",
    "\n",
    "def generate_commit_message(message):\n",
    "    return f'Added function to print \"{message}\"'\n",
    "\n",
    "\n",
    "def generate_all_combinations():\n",
    "    dataset = []\n",
    "    file_version = {file_name: 0 for file_name in file_names}  # Initialize version tracking\n",
    "\n",
    "    for file_name, function_name, message in itertools.product(\n",
    "        file_names, function_names, printed_messages\n",
    "    ):\n",
    "        file_version[file_name] += 1\n",
    "        version = file_version[file_name]\n",
    "\n",
    "        file_content = generate_file_content(function_name, message)\n",
    "\n",
    "        commit_message = generate_commit_message(message)\n",
    "\n",
    "        dataset.append(\n",
    "            {\n",
    "                \"file_name\": file_name,\n",
    "                \"version\": version,\n",
    "                \"commit_diff\": file_content,\n",
    "                \"commit_message\": commit_message,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "print(\"Generating all possible combinations. This may take a while...\")\n",
    "dataset = generate_all_combinations()\n",
    "print(f\"Total combinations generated: {len(dataset)}\")\n",
    "with open(OUTPUT_DIR / \"commit_dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset, f, indent=2)\n",
    "print(\"Dataset generated and saved to commit_dataset.json\")\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from src.data.types import SingleExample\n",
    "\n",
    "dataset = load_from_disk(OUTPUT_DIR / \"02-processed-validation\")\n",
    "dataset.select(range(10)).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "commit_diffs = df[\"commit_diff\"].tolist() + df[\"commit_message\"].tolist()\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "special_tokens = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=3000, special_tokens=special_tokens)\n",
    "\n",
    "tokenizer.train_from_iterator(commit_diffs, trainer=trainer)\n",
    "\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    pair=\"<s> $A </s> </s> $B </s>\",\n",
    "    special_tokens=[\n",
    "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "tokenizer.save(str(OUTPUT_DIR / \"bpe_tokenizer.json\"))\n",
    "\n",
    "tokenizer = Tokenizer.from_file(str(OUTPUT_DIR / \"bpe_tokenizer.json\"))\n",
    "\n",
    "\n",
    "def encode_text(text, max_length):\n",
    "    encoding = tokenizer.encode(text)\n",
    "    if len(encoding.ids) > max_length:\n",
    "        encoding = encoding.slice(0, max_length)\n",
    "    else:\n",
    "        encoding.pad(length=max_length)\n",
    "    return encoding.ids\n",
    "\n",
    "\n",
    "max_input_length = 100\n",
    "max_target_length = 100\n",
    "\n",
    "input_ids = [encode_text(diff, max_input_length) for diff in df[\"commit_diff\"].tolist()]\n",
    "target_ids = [encode_text(msg, max_target_length) for msg in df[\"commit_message\"].tolist()]\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "target_ids = torch.tensor(target_ids)\n",
    "\n",
    "dataset_size = len(df)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_inputs, train_targets = input_ids[train_indices], target_ids[train_indices]\n",
    "test_inputs, test_targets = input_ids[test_indices], target_ids[test_indices]\n",
    "\n",
    "print(f\"Training samples: {len(train_inputs)}\")\n",
    "print(f\"Testing samples: {len(test_inputs)}\")\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(\n",
    "            self._generate_positional_encoding(5000, embed_size), requires_grad=False\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_size, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _generate_positional_encoding(self, max_len, embed_size):\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2) * (-np.log(10000.0) / embed_size))\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.token_embedding(src) + self.positional_encoding[: src.size(1), :]\n",
    "        src = self.dropout(src)\n",
    "        memory = self.transformer_encoder(src.permute(1, 0, 2), src_mask)\n",
    "        output = self.fc_out(memory.permute(1, 0, 2))\n",
    "        return output\n",
    "\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embed_size = 256\n",
    "num_heads = 8\n",
    "hidden_dim = 512\n",
    "num_layers = 3\n",
    "dropout = 0.1\n",
    "\n",
    "model = TransformerModel(vocab_size, embed_size, num_heads, hidden_dim, num_layers, dropout).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "\n",
    "class CommitDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.inputs[idx], \"target_ids\": self.targets[idx]}\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_dataset = CommitDataset(train_inputs, train_targets)\n",
    "test_dataset = CommitDataset(test_inputs, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "pad_token_id = tokenizer.token_to_id(\"<pad>\")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        target_ids = batch[\"target_ids\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, src_mask=None)\n",
    "\n",
    "        output = output.contiguous().view(-1, vocab_size)  # (batch_size * seq_len, vocab_size)\n",
    "        target = target_ids.contiguous().view(-1)  # (batch_size * seq_len)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            target_ids = batch[\"target_ids\"].to(device)\n",
    "\n",
    "            output = model(input_ids, src_mask=None)\n",
    "\n",
    "            output = output.contiguous().view(-1, vocab_size)  # (batch_size * seq_len, vocab_size)\n",
    "            target = target_ids.contiguous().view(-1)  # (batch_size * seq_len)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, test_loader, criterion, device)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_commit_message_(model, tokenizer, commit_diff, max_length=64):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoding = tokenizer.encode(commit_diff)\n",
    "        if len(encoding.ids) > max_input_length:\n",
    "            encoding = encoding.slice(0, max_input_length)\n",
    "        else:\n",
    "            encoding.pad(length=max_length)  # Pad in place\n",
    "        input_ids = torch.tensor([encoding.ids]).to(device)\n",
    "\n",
    "        outputs = model(input_ids)\n",
    "        predicted_ids = outputs.argmax(dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "        decoded = tokenizer.decode(predicted_ids)\n",
    "        decoded = decoded.replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "        return decoded\n",
    "\n",
    "\n",
    "new_commit_diff = '''def list_users():\n",
    "    \"\"\"Lists all users in the system.\"\"\"\n",
    "    return database.find_all()\n",
    "'''\n",
    "\n",
    "generated_message = generate_commit_message_(model, tokenizer, new_commit_diff)\n",
    "print(f\"Commit Diff:\\n{new_commit_diff}\\n\")\n",
    "print(f\"Generated Commit Message: {generated_message}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
