{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a643813c04681d7",
   "metadata": {},
   "source": [
    "# Data Collator Experiments\n",
    "\n",
    "In this notebook, we'll explore how to construct batches out of processed `Commit Chronicle` dataset during the training/validation setting for a encoder-decoder style architecture.\n",
    "\n",
    "**Make sure to run `commit-chronicle-dataset.ipynb` before using this notebook.**\n",
    "\n",
    "The logic laid out in this notebook is implemented in `DataCollatorTrain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rootutils\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271185ef40822a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = rootutils.setup_root(\".\", \".project-root\", pythonpath=True)\n",
    "OUTPUT_DIR = ROOT / \"data/playground\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1664dd27-565d-419e-84bc-a81fc523fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.types import SingleExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66560668fa852eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = load_from_disk(OUTPUT_DIR / \"02-processed-validation\")\n",
    "dataset_.select(range(10)).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21977b087089398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumbleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index: int) -> SingleExample:\n",
    "        row = self.dataset[index]\n",
    "        return SingleExample(\n",
    "            diff_input_ids=row[\"diff_input_ids\"],\n",
    "            msg_input_ids=row[\"msg_input_ids\"],\n",
    "            history_input_ids=[],  # ignored in this notebook. don't worry about it. trust me :)\n",
    "            pos_in_file=-1,  # ignored in this notebook.\n",
    "        )\n",
    "\n",
    "\n",
    "data = HumbleDataset(dataset_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d86b05f-60b8-4fa2-b1f0-8d9d3350c97a",
   "metadata": {},
   "source": [
    "Let's load the tokenizers we used in `commit-chronicle-dataset.ipynb`. We are done tokenizing but we need to known the identities of some special tokens, i.e. beginning of sentence (BOS) token, end of sentence (EOS) token, and the pad token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18fd5dede64a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.data.components.tokenization import add_special_tokens\n",
    "\n",
    "msg_tokenizer_ = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
    "msg_tokenizer_ = add_special_tokens(msg_tokenizer_, None)\n",
    "diff_tokenizer_ = deepcopy(msg_tokenizer_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab645ac76704b58",
   "metadata": {},
   "source": [
    "# Encoder Input Processing\n",
    "\n",
    "Here we assume input to the encoder is the git diff, `diff_input_ids` attribute of `SingleExample`. It can also be history of all git diffs, but we don't use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5347e3cf34422c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoder_inputs(\n",
    "    input_ids_batch: list[list[int]],\n",
    "    encoder_context_max_len: int,\n",
    "    bos_token_id: int,\n",
    "    eos_token_id: int,\n",
    "    pad_token_id: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This helper method processes either diffs or messages as encoder input.\n",
    "\n",
    "    It truncates the inputs to the maximum allowed length.\n",
    "\n",
    "    It also adds all required special tokens: format is [BOS] input [EOS].\n",
    "\n",
    "    Finally, it is responsible for padding to maximum length in batch and conversion to torch.Tensor.\n",
    "\n",
    "    Args:\n",
    "        input_ids_batch: A list of tokenized examples from the current batch.\n",
    "        encoder_context_max_len: The maximum length of the encoder context.\n",
    "        bos_token_id: The value of the beginning of sequence (BOS) token.\n",
    "        eos_token_id: The value of the end of sequence (EOS) token.\n",
    "        pad_token_id: The value of the padding token (PAD) token.\n",
    "\n",
    "    Returns:\n",
    "        input_ids for encoder, attention_mask for encoder\n",
    "    \"\"\"\n",
    "\n",
    "    # add BOS and EOS tokens to each example whilst making sure max length of resulting token list is encoder_context_max_len\n",
    "    input_ids_batch = [\n",
    "        [bos_token_id] + example[: encoder_context_max_len - 2] + [eos_token_id]\n",
    "        for example in input_ids_batch\n",
    "    ]\n",
    "    inputs_tensors = [torch.tensor(ids, dtype=torch.int64) for ids in input_ids_batch]\n",
    "\n",
    "    # pad tensors to max length in batch\n",
    "    inputs_max_len = max(len(tensor) for tensor in input_ids_batch)\n",
    "    inputs_tensors = [\n",
    "        _pad_tensor(\n",
    "            tensor,\n",
    "            pad_len=inputs_max_len - tensor.numel(),\n",
    "            value=pad_token_id,\n",
    "            left=False,\n",
    "        )\n",
    "        for tensor in inputs_tensors\n",
    "    ]\n",
    "\n",
    "    masks_tensors = [torch.ones_like(ids) for ids in inputs_tensors]\n",
    "    masks_tensors = [\n",
    "        _pad_tensor(\n",
    "            tensor,\n",
    "            pad_len=inputs_max_len - tensor.numel(),\n",
    "            value=0,\n",
    "            left=False,\n",
    "        )\n",
    "        for tensor in masks_tensors\n",
    "    ]\n",
    "    return torch.stack(inputs_tensors), torch.stack(masks_tensors)\n",
    "\n",
    "\n",
    "def _pad_tensor(input_tensor: torch.Tensor, pad_len: int, value: int, left: bool) -> torch.Tensor:\n",
    "    return torch.nn.functional.pad(\n",
    "        input_tensor,\n",
    "        pad=[pad_len, 0] if left else [0, pad_len],\n",
    "        mode=\"constant\",\n",
    "        value=value,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7aa941fcb1555",
   "metadata": {},
   "source": [
    "Let's try it out with a batch size of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54c9d6da4cdf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_ = [data[0], data[1]]\n",
    "git_diff_inputs_ = [example.diff_input_ids for example in examples_]\n",
    "encoder_input_ids_, encoder_attention_mask_ = process_encoder_inputs(\n",
    "    input_ids_batch=git_diff_inputs_,\n",
    "    encoder_context_max_len=512,  # this is a hyperparameter\n",
    "    bos_token_id=diff_tokenizer_.bos_token_id,\n",
    "    eos_token_id=diff_tokenizer_.eos_token_id,\n",
    "    pad_token_id=diff_tokenizer_.pad_token_id,\n",
    ")\n",
    "encoder_input_ids_.shape, encoder_attention_mask_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89460b619b8d24fc",
   "metadata": {},
   "source": [
    "That's it. The output data forms input to our encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb51094f1f0c486b",
   "metadata": {},
   "source": [
    "# Decoder Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c47d6d0e78d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "\n",
    "\n",
    "def _process_decoder_input(\n",
    "    examples: list[SingleExample],\n",
    "    msg_bos_token_id: int,\n",
    "    msg_eos_token_id: int,\n",
    "    msg_pad_token_id: int,\n",
    "    decoder_context_max_len,\n",
    "    shift_labels: bool,\n",
    "    decoder_start_token_id: Optional[int] = None,\n",
    "    # ignore these options\n",
    "    encoder_input_type: Literal[\"diff\", \"history\"] = \"diff\",\n",
    "    with_history: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepares decoder input for train/validation:\n",
    "      * aggregates messages from history when configured accordingly\n",
    "      * concatenates history with current message\n",
    "      * constructs labels\n",
    "      * pads, converts to tensors\n",
    "\n",
    "    Args:\n",
    "        examples: A list of inputs for current batch.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of three tensors: input ids, attention masks, labels.\n",
    "    \"\"\"\n",
    "    message_inputs: list[list[int]] = [example.msg_input_ids for example in examples]\n",
    "    history_inputs: list[list[list[int]]] = [example.history_input_ids for example in examples]\n",
    "\n",
    "    all_msg_ids: list[torch.Tensor] = []\n",
    "    all_msg_masks: list[torch.Tensor] = []\n",
    "    all_msg_labels: list[torch.Tensor] = []\n",
    "\n",
    "    for message_ids, history_ids in zip(message_inputs, history_inputs):\n",
    "        message_ids = message_ids[: decoder_context_max_len - 2]\n",
    "\n",
    "        cur_history_ids = []\n",
    "        cur_history_labels = []\n",
    "\n",
    "        # if encoder_input_type != \"history\" and with_history:\n",
    "        #     cur_history_ids = _get_history(\n",
    "        #         cur_len=len(message_ids) + 2,\n",
    "        #         history_ids=history_ids,\n",
    "        #     )\n",
    "        #     cur_history_labels = [\n",
    "        #         [-100 for _ in message] for message in cur_history_ids\n",
    "        #     ]\n",
    "\n",
    "        cur_ids = [[msg_bos_token_id]] + cur_history_ids + [message_ids] + [[msg_eos_token_id]]\n",
    "        cur_labels = (\n",
    "            [[msg_bos_token_id]] + cur_history_labels + [message_ids] + [[msg_eos_token_id]]\n",
    "        )\n",
    "\n",
    "        if shift_labels:\n",
    "            cur_ids, cur_labels = _shift_for_encoder_decoder(\n",
    "                cur_ids,\n",
    "                cur_labels,\n",
    "                msg_bos_token_id=msg_bos_token_id,\n",
    "                decoder_start_token_id=decoder_start_token_id,\n",
    "            )\n",
    "\n",
    "        cur_ids_tensor = torch.tensor(\n",
    "            [ex for sublist in cur_ids for ex in sublist], dtype=torch.int64\n",
    "        )\n",
    "        cur_labels_tensor = torch.tensor(\n",
    "            [ex for sublist in cur_labels for ex in sublist], dtype=torch.int64\n",
    "        )\n",
    "        cur_mask_tensor = torch.ones_like(cur_ids_tensor)\n",
    "\n",
    "        all_msg_ids.append(cur_ids_tensor)\n",
    "        all_msg_masks.append(cur_mask_tensor)\n",
    "        all_msg_labels.append(cur_labels_tensor)\n",
    "\n",
    "    msg_max_len = max(len(tensor) for tensor in all_msg_ids)\n",
    "    all_msg_ids = [\n",
    "        _pad_tensor(\n",
    "            tensor,\n",
    "            pad_len=msg_max_len - tensor.numel(),\n",
    "            value=msg_pad_token_id,\n",
    "            left=False,\n",
    "        )\n",
    "        for tensor in all_msg_ids\n",
    "    ]\n",
    "    all_msg_masks = [\n",
    "        _pad_tensor(\n",
    "            tensor,\n",
    "            pad_len=msg_max_len - tensor.numel(),\n",
    "            value=0,\n",
    "            left=False,\n",
    "        )\n",
    "        for tensor in all_msg_masks\n",
    "    ]\n",
    "    all_msg_labels = [\n",
    "        _pad_tensor(\n",
    "            tensor,\n",
    "            pad_len=msg_max_len - tensor.numel(),\n",
    "            value=-100,\n",
    "            left=False,\n",
    "        )\n",
    "        for tensor in all_msg_labels\n",
    "    ]\n",
    "\n",
    "    return (\n",
    "        torch.stack(all_msg_ids),\n",
    "        torch.stack(all_msg_masks),\n",
    "        torch.stack(all_msg_labels),\n",
    "    )\n",
    "\n",
    "\n",
    "def _shift_for_encoder_decoder(\n",
    "    ids: list[list[int]],\n",
    "    labels: list[list[int]],\n",
    "    msg_bos_token_id: int,\n",
    "    decoder_start_token_id: Optional[int] = None,\n",
    ") -> tuple[list[list[int]], list[list[int]]]:\n",
    "    \"\"\"This method mimics transformers logic of ids and labels for EncoderDecoderModel\n",
    "    (or T5ForConditionalGeneration).\n",
    "\n",
    "    Starting from transformers v4.12, loss is now calculated in EncoderDecoderModel, not in decoder class.\n",
    "    Also, decoder input ids are created automatically based on labels: labels are shifted and -100 is replaced\n",
    "    with pad token. In our case, history ids are masked -100 in labels, but they are still\n",
    "    meaningful ids. Therefore, we can't use the default approach.\n",
    "    \"\"\"\n",
    "    if decoder_start_token_id is None:\n",
    "        ids = [[msg_bos_token_id]] + ids[:-1]\n",
    "    else:\n",
    "        ids = [[decoder_start_token_id]] + ids[:-1]\n",
    "    return ids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d112c8309ce4be49",
   "metadata": {},
   "source": [
    "Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd125d3780d419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ids_, decoder_attention_mask_, labels_ = _process_decoder_input(\n",
    "    examples=examples_,\n",
    "    msg_bos_token_id=msg_tokenizer_.bos_token_id,\n",
    "    msg_eos_token_id=msg_tokenizer_.eos_token_id,\n",
    "    msg_pad_token_id=msg_tokenizer_.pad_token_id,\n",
    "    decoder_context_max_len=512,  # this is a hyperparam for the model\n",
    "    shift_labels=True,\n",
    ")\n",
    "decoder_input_ids_.shape, decoder_attention_mask_.shape, labels_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caff40753de02be",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff07ec03fcb9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ffd6afe276cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(\n",
    "    input_ids=encoder_input_ids_,\n",
    "    attention_mask=encoder_attention_mask_,\n",
    "    decoder_input_ids=decoder_input_ids_,\n",
    "    decoder_attention_mask=decoder_attention_mask_,\n",
    "    labels=labels_,\n",
    ")\n",
    "[attr for attr in dir(outputs) if not attr.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccdd890a07d4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174b52a4311ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's overfit\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\").train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=0.1)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(\n",
    "        input_ids=encoder_input_ids_,\n",
    "        attention_mask=encoder_attention_mask_,\n",
    "        decoder_input_ids=decoder_input_ids_,\n",
    "        decoder_attention_mask=decoder_attention_mask_,\n",
    "        labels=labels_,\n",
    "    )\n",
    "\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {i:03d} Loss:{loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e21c29bbf9097",
   "metadata": {},
   "source": [
    "I was expecting the model loss to reduce smoothly but that didn't happen. Hmmmmm..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
