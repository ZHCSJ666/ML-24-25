{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a643813c04681d7",
   "metadata": {},
   "source": [
    "# Data Collator Experiments\n",
    "\n",
    "In this notebook, we'll explore how to construct batches out of processed `Commit Chronicle` dataset during the training/validation setting for a encoder-decoder style architecture.\n",
    "\n",
    "**Make sure to run `commit-chronicle-dataset.ipynb` before using this notebook.**\n",
    "\n",
    "The logic laid out in this notebook is implemented in `DataCollatorTrain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:42.359455Z",
     "start_time": "2024-11-09T07:11:39.362450Z"
    }
   },
   "outputs": [],
   "source": [
    "import rootutils\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8271185ef40822a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:42.365553Z",
     "start_time": "2024-11-09T07:11:42.361964Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT = rootutils.setup_root(\".\", \".project-root\", pythonpath=True)\n",
    "OUTPUT_DIR = ROOT / \"data/playground\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1664dd27-565d-419e-84bc-a81fc523fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.types import SingleExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b66560668fa852eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:42.739422Z",
     "start_time": "2024-11-09T07:11:42.569554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>msg_input_ids</th>\n",
       "      <th>diff_input_ids</th>\n",
       "      <th>language</th>\n",
       "      <th>repo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>770513</td>\n",
       "      <td>[986, 981, 358, 4614, 326, 3434, 729, 1867, 60...</td>\n",
       "      <td>[2704, 585, 2713, 19, 1425, 19, 1425, 18, 3240...</td>\n",
       "      <td>Go</td>\n",
       "      <td>bios-marcel/cordless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>770513</td>\n",
       "      <td>[986, 1390, 364, 5023, 326, 2521, 471, 19171, ...</td>\n",
       "      <td>[2704, 585, 276, 517, 2656, 18, 3240, 203, 15,...</td>\n",
       "      <td>Go</td>\n",
       "      <td>bios-marcel/cordless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>770513</td>\n",
       "      <td>[986, 279, 24778, 716, 22991, 4167, 326, 4422,...</td>\n",
       "      <td>[2704, 585, 10746, 958, 18, 1264, 203, 15, 7, ...</td>\n",
       "      <td>Go</td>\n",
       "      <td>bios-marcel/cordless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>770513</td>\n",
       "      <td>[986, 3270, 358, 1086, 707]</td>\n",
       "      <td>[7236, 19, 2910, 18, 3240, 203, 30989, 300, 26...</td>\n",
       "      <td>Go</td>\n",
       "      <td>bios-marcel/cordless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>770513</td>\n",
       "      <td>[5726, 2172, 7153, 16, 29288, 364, 946, 310, 4...</td>\n",
       "      <td>[6949, 958, 18, 1264, 203, 30989, 300, 5558, 1...</td>\n",
       "      <td>Go</td>\n",
       "      <td>bios-marcel/cordless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>770513</td>\n",
       "      <td>[6464, 4677, 461, 3152, 6810]</td>\n",
       "      <td>[7236, 19, 1425, 19, 1425, 18, 3240, 203, 3098...</td>\n",
       "      <td>Go</td>\n",
       "      <td>bios-marcel/cordless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>770513</td>\n",
       "      <td>[986, 1122, 4409, 18, 203, 1986, 4409, 1914, 7...</td>\n",
       "      <td>[7236, 19, 2910, 18, 3240, 203, 30989, 300, 27...</td>\n",
       "      <td>Go</td>\n",
       "      <td>bios-marcel/cordless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>770513</td>\n",
       "      <td>[7505, 279, 7934, 1625, 777, 2743, 434, 279, 1...</td>\n",
       "      <td>[7236, 19, 4881, 19, 5668, 18, 3240, 203, 3098...</td>\n",
       "      <td>Go</td>\n",
       "      <td>bios-marcel/cordless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>770513</td>\n",
       "      <td>[5058, 854, 2037, 777, 4203, 16, 309, 1915, 45...</td>\n",
       "      <td>[7236, 19, 4881, 19, 5668, 18, 3240, 203, 3098...</td>\n",
       "      <td>Go</td>\n",
       "      <td>bios-marcel/cordless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>770513</td>\n",
       "      <td>[8585, 1904, 7153, 16, 2037, 777, 2743, 854, 4...</td>\n",
       "      <td>[7236, 19, 4881, 19, 5668, 18, 3240, 203, 3098...</td>\n",
       "      <td>Go</td>\n",
       "      <td>bios-marcel/cordless</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author                                      msg_input_ids  \\\n",
       "0  770513  [986, 981, 358, 4614, 326, 3434, 729, 1867, 60...   \n",
       "1  770513  [986, 1390, 364, 5023, 326, 2521, 471, 19171, ...   \n",
       "2  770513  [986, 279, 24778, 716, 22991, 4167, 326, 4422,...   \n",
       "3  770513                        [986, 3270, 358, 1086, 707]   \n",
       "4  770513  [5726, 2172, 7153, 16, 29288, 364, 946, 310, 4...   \n",
       "5  770513                      [6464, 4677, 461, 3152, 6810]   \n",
       "6  770513  [986, 1122, 4409, 18, 203, 1986, 4409, 1914, 7...   \n",
       "7  770513  [7505, 279, 7934, 1625, 777, 2743, 434, 279, 1...   \n",
       "8  770513  [5058, 854, 2037, 777, 4203, 16, 309, 1915, 45...   \n",
       "9  770513  [8585, 1904, 7153, 16, 2037, 777, 2743, 854, 4...   \n",
       "\n",
       "                                      diff_input_ids language  \\\n",
       "0  [2704, 585, 2713, 19, 1425, 19, 1425, 18, 3240...       Go   \n",
       "1  [2704, 585, 276, 517, 2656, 18, 3240, 203, 15,...       Go   \n",
       "2  [2704, 585, 10746, 958, 18, 1264, 203, 15, 7, ...       Go   \n",
       "3  [7236, 19, 2910, 18, 3240, 203, 30989, 300, 26...       Go   \n",
       "4  [6949, 958, 18, 1264, 203, 30989, 300, 5558, 1...       Go   \n",
       "5  [7236, 19, 1425, 19, 1425, 18, 3240, 203, 3098...       Go   \n",
       "6  [7236, 19, 2910, 18, 3240, 203, 30989, 300, 27...       Go   \n",
       "7  [7236, 19, 4881, 19, 5668, 18, 3240, 203, 3098...       Go   \n",
       "8  [7236, 19, 4881, 19, 5668, 18, 3240, 203, 3098...       Go   \n",
       "9  [7236, 19, 4881, 19, 5668, 18, 3240, 203, 3098...       Go   \n",
       "\n",
       "                   repo  \n",
       "0  bios-marcel/cordless  \n",
       "1  bios-marcel/cordless  \n",
       "2  bios-marcel/cordless  \n",
       "3  bios-marcel/cordless  \n",
       "4  bios-marcel/cordless  \n",
       "5  bios-marcel/cordless  \n",
       "6  bios-marcel/cordless  \n",
       "7  bios-marcel/cordless  \n",
       "8  bios-marcel/cordless  \n",
       "9  bios-marcel/cordless  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ = load_from_disk(OUTPUT_DIR / \"02-processed-validation\")\n",
    "dataset_.select(range(10)).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21977b087089398c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:42.777409Z",
     "start_time": "2024-11-09T07:11:42.773621Z"
    }
   },
   "outputs": [],
   "source": [
    "class HumbleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index: int) -> SingleExample:\n",
    "        row = self.dataset[index]\n",
    "        return SingleExample(\n",
    "            diff_input_ids=row[\"diff_input_ids\"],\n",
    "            msg_input_ids=row[\"msg_input_ids\"],\n",
    "            history_input_ids=[],  # ignored in this notebook. don't worry about it. trust me :)\n",
    "            pos_in_file=-1,  # ignored in this notebook.\n",
    "        )\n",
    "\n",
    "\n",
    "data = HumbleDataset(dataset_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d86b05f-60b8-4fa2-b1f0-8d9d3350c97a",
   "metadata": {},
   "source": [
    "Let's load the tokenizers we used in `commit-chronicle-dataset.ipynb`. We are done tokenizing but we need to known the identities of some special tokens, i.e. beginning of sentence (BOS) token, end of sentence (EOS) token, and the pad token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d18fd5dede64a7c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:44.009361Z",
     "start_time": "2024-11-09T07:11:42.826058Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.data.components.tokenization import add_special_tokens\n",
    "from transformers import AutoTokenizer\n",
    "from copy import deepcopy\n",
    "\n",
    "msg_tokenizer_ = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
    "msg_tokenizer_ = add_special_tokens(msg_tokenizer_, None)\n",
    "diff_tokenizer_ = deepcopy(msg_tokenizer_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab645ac76704b58",
   "metadata": {},
   "source": [
    "# Encoder Input Processing\n",
    "\n",
    "Here we assume input to the encoder is the git diff, `diff_input_ids` attribute of `SingleExample`. It can also be history of all git diffs, but we don't use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5347e3cf34422c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:44.018397Z",
     "start_time": "2024-11-09T07:11:44.012358Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_encoder_inputs(\n",
    "    input_ids_batch: list[list[int]],\n",
    "    encoder_context_max_len: int,\n",
    "    bos_token_id: int,\n",
    "    eos_token_id: int,\n",
    "    pad_token_id: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This helper method processes either diffs or messages as encoder input.\n",
    "\n",
    "    It truncates the inputs to the maximum allowed length.\n",
    "\n",
    "    It also adds all required special tokens: format is [BOS] input [EOS].\n",
    "\n",
    "    Finally, it is responsible for padding to maximum length in batch and conversion to torch.Tensor.\n",
    "\n",
    "    Args:\n",
    "        input_ids_batch: A list of tokenized examples from the current batch.\n",
    "        encoder_context_max_len: The maximum length of the encoder context.\n",
    "        bos_token_id: The value of the beginning of sequence (BOS) token.\n",
    "        eos_token_id: The value of the end of sequence (EOS) token.\n",
    "        pad_token_id: The value of the padding token (PAD) token.\n",
    "\n",
    "    Returns:\n",
    "        input_ids for encoder, attention_mask for encoder\n",
    "    \"\"\"\n",
    "\n",
    "    # add BOS and EOS tokens to each example whilst making sure max length of resulting token list is encoder_context_max_len\n",
    "    input_ids_batch = [\n",
    "        [bos_token_id] + example[: encoder_context_max_len - 2] + [eos_token_id]\n",
    "        for example in input_ids_batch\n",
    "    ]\n",
    "    inputs_tensors = [torch.tensor(ids, dtype=torch.int64) for ids in input_ids_batch]\n",
    "\n",
    "    # pad tensors to max length in batch\n",
    "    inputs_max_len = max(len(tensor) for tensor in input_ids_batch)\n",
    "    inputs_tensors = [\n",
    "        _pad_tensor(\n",
    "            tensor,\n",
    "            pad_len=inputs_max_len - tensor.numel(),\n",
    "            value=pad_token_id,\n",
    "            left=False,\n",
    "        )\n",
    "        for tensor in inputs_tensors\n",
    "    ]\n",
    "\n",
    "    masks_tensors = [torch.ones_like(ids) for ids in inputs_tensors]\n",
    "    masks_tensors = [\n",
    "        _pad_tensor(\n",
    "            tensor,\n",
    "            pad_len=inputs_max_len - tensor.numel(),\n",
    "            value=0,\n",
    "            left=False,\n",
    "        )\n",
    "        for tensor in masks_tensors\n",
    "    ]\n",
    "    return torch.stack(inputs_tensors), torch.stack(masks_tensors)\n",
    "\n",
    "\n",
    "def _pad_tensor(\n",
    "    input_tensor: torch.Tensor, pad_len: int, value: int, left: bool\n",
    ") -> torch.Tensor:\n",
    "    return torch.nn.functional.pad(\n",
    "        input_tensor,\n",
    "        pad=[pad_len, 0] if left else [0, pad_len],\n",
    "        mode=\"constant\",\n",
    "        value=value,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7aa941fcb1555",
   "metadata": {},
   "source": [
    "Let's try it out with a batch size of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc54c9d6da4cdf12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:44.037073Z",
     "start_time": "2024-11-09T07:11:44.031102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 512]), torch.Size([2, 512]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_ = [data[0], data[1]]\n",
    "git_diff_inputs_ = [example.diff_input_ids for example in examples_]\n",
    "encoder_input_ids_, encoder_attention_mask_ = process_encoder_inputs(\n",
    "    input_ids_batch=git_diff_inputs_,\n",
    "    encoder_context_max_len=512,  # this is a hyperparameter\n",
    "    bos_token_id=diff_tokenizer_.bos_token_id,\n",
    "    eos_token_id=diff_tokenizer_.eos_token_id,\n",
    "    pad_token_id=diff_tokenizer_.pad_token_id,\n",
    ")\n",
    "encoder_input_ids_.shape, encoder_attention_mask_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89460b619b8d24fc",
   "metadata": {},
   "source": [
    "That's it. The output data forms input to our encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb51094f1f0c486b",
   "metadata": {},
   "source": [
    "# Decoder Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d1c47d6d0e78d46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:44.063797Z",
     "start_time": "2024-11-09T07:11:44.053431Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "\n",
    "\n",
    "def _process_decoder_input(\n",
    "    examples: list[SingleExample],\n",
    "    msg_bos_token_id: int,\n",
    "    msg_eos_token_id: int,\n",
    "    msg_pad_token_id: int,\n",
    "    decoder_context_max_len,\n",
    "    shift_labels: bool,\n",
    "    decoder_start_token_id: Optional[int] = None,\n",
    "    # ignore these options\n",
    "    encoder_input_type: Literal[\"diff\", \"history\"] = \"diff\",\n",
    "    with_history: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepares decoder input for train/validation:\n",
    "      * aggregates messages from history when configured accordingly\n",
    "      * concatenates history with current message\n",
    "      * constructs labels\n",
    "      * pads, converts to tensors\n",
    "\n",
    "    Args:\n",
    "        examples: A list of inputs for current batch.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of three tensors: input ids, attention masks, labels.\n",
    "    \"\"\"\n",
    "    message_inputs: list[list[int]] = [example.msg_input_ids for example in examples]\n",
    "    history_inputs: list[list[list[int]]] = [\n",
    "        example.history_input_ids for example in examples\n",
    "    ]\n",
    "\n",
    "    all_msg_ids: list[torch.Tensor] = []\n",
    "    all_msg_masks: list[torch.Tensor] = []\n",
    "    all_msg_labels: list[torch.Tensor] = []\n",
    "\n",
    "    for message_ids, history_ids in zip(message_inputs, history_inputs):\n",
    "        message_ids = message_ids[: decoder_context_max_len - 2]\n",
    "\n",
    "        cur_history_ids = []\n",
    "        cur_history_labels = []\n",
    "\n",
    "        # if encoder_input_type != \"history\" and with_history:\n",
    "        #     cur_history_ids = _get_history(\n",
    "        #         cur_len=len(message_ids) + 2,\n",
    "        #         history_ids=history_ids,\n",
    "        #     )\n",
    "        #     cur_history_labels = [\n",
    "        #         [-100 for _ in message] for message in cur_history_ids\n",
    "        #     ]\n",
    "\n",
    "        cur_ids = (\n",
    "            [[msg_bos_token_id]]\n",
    "            + cur_history_ids\n",
    "            + [message_ids]\n",
    "            + [[msg_eos_token_id]]\n",
    "        )\n",
    "        cur_labels = (\n",
    "            [[msg_bos_token_id]]\n",
    "            + cur_history_labels\n",
    "            + [message_ids]\n",
    "            + [[msg_eos_token_id]]\n",
    "        )\n",
    "\n",
    "        if shift_labels:\n",
    "            cur_ids, cur_labels = _shift_for_encoder_decoder(\n",
    "                cur_ids,\n",
    "                cur_labels,\n",
    "                msg_bos_token_id=msg_bos_token_id,\n",
    "                decoder_start_token_id=decoder_start_token_id,\n",
    "            )\n",
    "\n",
    "        cur_ids_tensor = torch.tensor(\n",
    "            [ex for sublist in cur_ids for ex in sublist], dtype=torch.int64\n",
    "        )\n",
    "        cur_labels_tensor = torch.tensor(\n",
    "            [ex for sublist in cur_labels for ex in sublist], dtype=torch.int64\n",
    "        )\n",
    "        cur_mask_tensor = torch.ones_like(cur_ids_tensor)\n",
    "\n",
    "        all_msg_ids.append(cur_ids_tensor)\n",
    "        all_msg_masks.append(cur_mask_tensor)\n",
    "        all_msg_labels.append(cur_labels_tensor)\n",
    "\n",
    "    msg_max_len = max(len(tensor) for tensor in all_msg_ids)\n",
    "    all_msg_ids = [\n",
    "        _pad_tensor(\n",
    "            tensor,\n",
    "            pad_len=msg_max_len - tensor.numel(),\n",
    "            value=msg_pad_token_id,\n",
    "            left=False,\n",
    "        )\n",
    "        for tensor in all_msg_ids\n",
    "    ]\n",
    "    all_msg_masks = [\n",
    "        _pad_tensor(\n",
    "            tensor,\n",
    "            pad_len=msg_max_len - tensor.numel(),\n",
    "            value=0,\n",
    "            left=False,\n",
    "        )\n",
    "        for tensor in all_msg_masks\n",
    "    ]\n",
    "    all_msg_labels = [\n",
    "        _pad_tensor(\n",
    "            tensor,\n",
    "            pad_len=msg_max_len - tensor.numel(),\n",
    "            value=-100,\n",
    "            left=False,\n",
    "        )\n",
    "        for tensor in all_msg_labels\n",
    "    ]\n",
    "\n",
    "    return (\n",
    "        torch.stack(all_msg_ids),\n",
    "        torch.stack(all_msg_masks),\n",
    "        torch.stack(all_msg_labels),\n",
    "    )\n",
    "\n",
    "\n",
    "def _shift_for_encoder_decoder(\n",
    "    ids: list[list[int]],\n",
    "    labels: list[list[int]],\n",
    "    msg_bos_token_id: int,\n",
    "    decoder_start_token_id: Optional[int] = None,\n",
    ") -> tuple[list[list[int]], list[list[int]]]:\n",
    "    \"\"\"This method mimics transformers logic of ids and labels for EncoderDecoderModel\n",
    "    (or T5ForConditionalGeneration).\n",
    "\n",
    "    Starting from transformers v4.12, loss is now calculated in EncoderDecoderModel, not in decoder class.\n",
    "    Also, decoder input ids are created automatically based on labels: labels are shifted and -100 is replaced\n",
    "    with pad token. In our case, history ids are masked -100 in labels, but they are still\n",
    "    meaningful ids. Therefore, we can't use the default approach.\n",
    "    \"\"\"\n",
    "    if decoder_start_token_id is None:\n",
    "        ids = [[msg_bos_token_id]] + ids[:-1]\n",
    "    else:\n",
    "        ids = [[decoder_start_token_id]] + ids[:-1]\n",
    "    return ids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d112c8309ce4be49",
   "metadata": {},
   "source": [
    "Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acd125d3780d419c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:44.082015Z",
     "start_time": "2024-11-09T07:11:44.077804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 26]), torch.Size([2, 26]), torch.Size([2, 26]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_ids_, decoder_attention_mask_, labels_ = _process_decoder_input(\n",
    "    examples=examples_,\n",
    "    msg_bos_token_id=msg_tokenizer_.bos_token_id,\n",
    "    msg_eos_token_id=msg_tokenizer_.eos_token_id,\n",
    "    msg_pad_token_id=msg_tokenizer_.pad_token_id,\n",
    "    decoder_context_max_len=512,  # this is a hyperparam for the model\n",
    "    shift_labels=True,\n",
    ")\n",
    "decoder_input_ids_.shape, decoder_attention_mask_.shape, labels_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caff40753de02be",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acff07ec03fcb9b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:46.378208Z",
     "start_time": "2024-11-09T07:11:44.095188Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "719ffd6afe276cbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:46.745202Z",
     "start_time": "2024-11-09T07:11:46.397566Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['clear',\n",
       " 'copy',\n",
       " 'cross_attentions',\n",
       " 'decoder_attentions',\n",
       " 'decoder_hidden_states',\n",
       " 'encoder_attentions',\n",
       " 'encoder_hidden_states',\n",
       " 'encoder_last_hidden_state',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'logits',\n",
       " 'loss',\n",
       " 'move_to_end',\n",
       " 'past_key_values',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'to_tuple',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(\n",
    "    input_ids=encoder_input_ids_,\n",
    "    attention_mask=encoder_attention_mask_,\n",
    "    decoder_input_ids=decoder_input_ids_,\n",
    "    decoder_attention_mask=decoder_attention_mask_,\n",
    "    labels=labels_,\n",
    ")\n",
    "[attr for attr in dir(outputs) if not attr.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ccdd890a07d4527",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:11:46.951214Z",
     "start_time": "2024-11-09T07:11:46.947128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.9106, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c174b52a4311ef92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:13:39.596713Z",
     "start_time": "2024-11-09T07:11:47.105166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 Loss:13.1849\n",
      "Epoch: 001 Loss:13.5523\n",
      "Epoch: 002 Loss:14.7886\n",
      "Epoch: 003 Loss:14.4896\n",
      "Epoch: 004 Loss:12.9214\n",
      "Epoch: 005 Loss:13.2874\n",
      "Epoch: 006 Loss:13.6502\n",
      "Epoch: 007 Loss:14.3785\n",
      "Epoch: 008 Loss:13.4003\n",
      "Epoch: 009 Loss:14.2756\n",
      "Epoch: 010 Loss:12.3113\n",
      "Epoch: 011 Loss:14.8840\n",
      "Epoch: 012 Loss:13.3760\n",
      "Epoch: 013 Loss:14.0161\n",
      "Epoch: 014 Loss:12.0281\n",
      "Epoch: 015 Loss:13.4774\n",
      "Epoch: 016 Loss:14.0890\n",
      "Epoch: 017 Loss:13.9686\n",
      "Epoch: 018 Loss:13.3837\n",
      "Epoch: 019 Loss:13.9526\n",
      "Epoch: 020 Loss:13.7633\n",
      "Epoch: 021 Loss:13.7142\n",
      "Epoch: 022 Loss:14.5893\n",
      "Epoch: 023 Loss:11.7874\n",
      "Epoch: 024 Loss:13.0091\n",
      "Epoch: 025 Loss:14.2026\n",
      "Epoch: 026 Loss:13.0280\n",
      "Epoch: 027 Loss:14.1758\n",
      "Epoch: 028 Loss:13.5117\n",
      "Epoch: 029 Loss:13.4204\n",
      "Epoch: 030 Loss:13.1268\n",
      "Epoch: 031 Loss:12.6725\n",
      "Epoch: 032 Loss:13.6725\n",
      "Epoch: 033 Loss:13.7256\n",
      "Epoch: 034 Loss:13.6830\n",
      "Epoch: 035 Loss:13.1339\n",
      "Epoch: 036 Loss:13.7265\n",
      "Epoch: 037 Loss:13.9796\n",
      "Epoch: 038 Loss:14.0493\n",
      "Epoch: 039 Loss:13.9367\n",
      "Epoch: 040 Loss:15.6461\n",
      "Epoch: 041 Loss:13.2513\n",
      "Epoch: 042 Loss:14.5230\n",
      "Epoch: 043 Loss:13.6694\n",
      "Epoch: 044 Loss:14.3018\n",
      "Epoch: 045 Loss:12.9260\n",
      "Epoch: 046 Loss:13.9747\n",
      "Epoch: 047 Loss:13.8539\n",
      "Epoch: 048 Loss:14.2114\n",
      "Epoch: 049 Loss:13.9045\n",
      "Epoch: 050 Loss:14.5412\n",
      "Epoch: 051 Loss:13.9424\n",
      "Epoch: 052 Loss:14.0086\n",
      "Epoch: 053 Loss:13.0897\n",
      "Epoch: 054 Loss:13.4434\n",
      "Epoch: 055 Loss:13.9588\n",
      "Epoch: 056 Loss:14.1961\n",
      "Epoch: 057 Loss:13.7721\n",
      "Epoch: 058 Loss:12.8493\n",
      "Epoch: 059 Loss:14.2785\n",
      "Epoch: 060 Loss:13.0648\n",
      "Epoch: 061 Loss:14.4184\n",
      "Epoch: 062 Loss:13.3449\n",
      "Epoch: 063 Loss:12.9424\n",
      "Epoch: 064 Loss:13.5195\n",
      "Epoch: 065 Loss:13.0228\n",
      "Epoch: 066 Loss:13.9368\n",
      "Epoch: 067 Loss:12.4910\n",
      "Epoch: 068 Loss:14.1801\n",
      "Epoch: 069 Loss:14.7746\n",
      "Epoch: 070 Loss:13.0749\n",
      "Epoch: 071 Loss:13.8489\n",
      "Epoch: 072 Loss:12.3619\n",
      "Epoch: 073 Loss:13.7600\n",
      "Epoch: 074 Loss:13.2013\n",
      "Epoch: 075 Loss:12.9559\n",
      "Epoch: 076 Loss:13.2573\n",
      "Epoch: 077 Loss:12.5694\n",
      "Epoch: 078 Loss:13.4852\n",
      "Epoch: 079 Loss:13.7743\n",
      "Epoch: 080 Loss:13.1012\n",
      "Epoch: 081 Loss:14.1118\n",
      "Epoch: 082 Loss:13.2395\n",
      "Epoch: 083 Loss:12.8258\n",
      "Epoch: 084 Loss:13.8824\n",
      "Epoch: 085 Loss:13.2385\n",
      "Epoch: 086 Loss:12.9565\n",
      "Epoch: 087 Loss:13.0578\n",
      "Epoch: 088 Loss:13.2416\n",
      "Epoch: 089 Loss:12.4730\n",
      "Epoch: 090 Loss:14.0018\n",
      "Epoch: 091 Loss:12.4575\n",
      "Epoch: 092 Loss:12.4775\n",
      "Epoch: 093 Loss:14.5091\n",
      "Epoch: 094 Loss:12.3226\n",
      "Epoch: 095 Loss:13.2481\n",
      "Epoch: 096 Loss:12.8550\n",
      "Epoch: 097 Loss:14.6212\n",
      "Epoch: 098 Loss:13.5251\n",
      "Epoch: 099 Loss:14.5135\n"
     ]
    }
   ],
   "source": [
    "# let's overfit\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\").train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=0.1)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(\n",
    "        input_ids=encoder_input_ids_,\n",
    "        attention_mask=encoder_attention_mask_,\n",
    "        decoder_input_ids=decoder_input_ids_,\n",
    "        decoder_attention_mask=decoder_attention_mask_,\n",
    "        labels=labels_,\n",
    "    )\n",
    "\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {i:03d} Loss:{loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e21c29bbf9097",
   "metadata": {},
   "source": [
    "I was expecting the model loss to reduce smoothly but that didn't happen. Hmmmmm..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac3a4768dba834c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T07:13:39.835864Z",
     "start_time": "2024-11-09T07:13:39.833651Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
