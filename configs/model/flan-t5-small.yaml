_target_: src.models.cmg_module.CommitMessageGenerationModule

optimizer:
  _target_: src.optimizers.create_optimizer
  _partial_: true
  lr: 1e-3
  weight_decay: 0.1

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 10

net:
  _target_: models.components.seq2seq_wrapper.Seq2SeqWrapper
  encoder_context_max_len: ???
  decoder_context_max_len: ???
  encoder_vocab_size: ???
  decoder_vocab_size: ???
  # This is flan-t5-small model introduced by google
  # This config was copied from https://huggingface.co/google/flan-t5-small/blob/main/config.json
  model:
    _target_: transformers.models.t5.modeling_t5.T5ForConditionalGeneration
    config:
      # See https://huggingface.co/docs/transformers/en/model_doc/t5#transformers.T5Config
      _target_: transformers.models.t5.configuration_t5.T5Config
      architectures:
        - T5ForConditionalGeneration
      d_ff: 1024
      d_kv: 64
      d_model: 512
      decoder_start_token_id: 0
      dropout_rate: 0.25 # TODO: revert
      eos_token_id: 1
      feed_forward_proj: gated-gelu
      initializer_factor: 1.0
      is_encoder_decoder: true
      layer_norm_epsilon: 1.0e-06
      model_type: t5
      n_positions: 512
      num_decoder_layers: 8
      num_heads: 6
      num_layers: 8
      output_past: true
      pad_token_id: 0
      relative_attention_max_distance: 128
      relative_attention_num_buckets: 32
      task_specific_params:
        summarization:
          early_stopping: true
          length_penalty: 2.0
          max_length: 200
          min_length: 30
          no_repeat_ngram_size: 3
          num_beams: 4
          prefix: 'summarize: '
        translation_en_to_de:
          early_stopping: true
          max_length: 300
          num_beams: 4
          prefix: 'translate English to German: '
        translation_en_to_fr:
          early_stopping: true
          max_length: 300
          num_beams: 4
          prefix: 'translate English to French: '
        translation_en_to_ro:
          early_stopping: true
          max_length: 300
          num_beams: 4
          prefix: 'translate English to Romanian: '
      tie_word_embeddings: false
      transformers_version: 4.23.1
      use_cache: true
      vocab_size: 32128


# compile model for faster training with pytorch 2.0
compile: false
