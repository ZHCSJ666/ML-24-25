# @package _global_

defaults:
  - override /data: commit-chronicle-seq2seq
  - override /model: baseline-cmg-codet5
  - override /callbacks: default
  - override /trainer: gpu
  - override /logger: wandb

tags: ["commit-chronicle", "codet5", "t5", "baseline"]

seed: 12345

trainer:
  min_epochs: 10
  max_epochs: 20
  precision: bf16-mixed

model:
  net:
    tokenizer: ${data.diff_tokenizer}
    decoder_tokenizer: ${data.msg_tokenizer}

data:
  change_types:
  batch_size: 16
  num_workers: 15
  # -1 to skip truncation
  diff_max_len: -1
  msg_max_len: -1
  diff_tokenizer:
    name_or_path: JetBrains-Research/cmg-codet5-without-history
  msg_tokenizer:
    name_or_path: JetBrains-Research/cmg-codet5-without-history
  # TODO: need to explain what these mean
  truncation_diff_max_len: 1008
  truncation_msg_max_len: 16
  truncation_diff_tokenizer:
    _target_: src.data.components.tokenization.load_tokenizer
    name_or_path: google/t5-efficient-tiny
    task: t5-truncate
  truncation_msg_tokenizer:
    _target_: src.data.components.tokenization.load_tokenizer
    name_or_path: google/t5-efficient-tiny
    task: t5-truncate

logger:
  wandb:
    name: baseline-cmg-codet5-without-history

# not interested in training baseline models
train: False

ckpt_path: